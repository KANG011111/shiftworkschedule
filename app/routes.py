from flask import Blueprint, render_template, request, jsonify, redirect, url_for, g, flash, session, Response
from app.models import db, Employee, ShiftType, Schedule, User, ImportLog, GroupMembers
from app.auth_middleware import require_auth, require_admin, optional_auth, get_current_user
from datetime import datetime, date
import pandas as pd
import json
import hashlib
import re
import os
import io
import csv
from collections import defaultdict

main = Blueprint('main', __name__)

@main.route('/')
@require_auth
def index():
    current_user = get_current_user()
    
    # å¼·åˆ¶æäº¤ä»»ä½•å¾…è™•ç†çš„è³‡æ–™åº«è®Šæ›´
    db.session.commit()
    
    today = date.today()
    
    # ç²å–ä»Šæ—¥æ’ç­ï¼Œæ’é™¤H0å’ŒH1ç­åˆ¥ï¼ˆä¼‘å‡ï¼‰ï¼ŒæŒ‰åŒ¯å…¥é †åºæ’åˆ—
    today_schedules = Schedule.query.join(ShiftType).filter(
        Schedule.date == today,
        ~ShiftType.code.in_(['H0', 'H1'])
    ).order_by(Schedule.import_order.asc().nullslast()).all()
    
    # å¦‚æœä»Šæ—¥æ²’æœ‰æ’ç­è¨˜éŒ„ï¼Œé¡¯ç¤ºæœ€è¿‘çš„æ’ç­è¨˜éŒ„ï¼ˆåŒæ¨£æ’é™¤H0å’ŒH1ï¼‰
    if not today_schedules:
        recent_schedules = Schedule.query.join(ShiftType).filter(
            ~ShiftType.code.in_(['H0', 'H1'])
        ).order_by(Schedule.date.desc(), Schedule.import_order.asc().nullslast()).limit(10).all()
        print(f"ğŸ” ä»Šæ—¥ç„¡æ’ç­è¨˜éŒ„ï¼Œé¡¯ç¤ºæœ€è¿‘ {len(recent_schedules)} ç­†æ’ç­è¨˜éŒ„ï¼ˆæ’é™¤ä¼‘å‡ï¼‰", flush=True)
        return render_template('index.html', today_schedules=recent_schedules, today=today, show_recent=True)
    
    # èª¿è©¦è¼¸å‡ºï¼šæª¢æŸ¥éæ¿¾çµæœ
    print(f"ğŸ” ä»Šæ—¥æ’ç­è¨˜éŒ„ (æ’é™¤H0/H1): {len(today_schedules)} ç­†", flush=True)
    for schedule in today_schedules:
        print(f"   {schedule.employee.name}: {schedule.shift_type.code}", flush=True)
    
    return render_template('index.html', today_schedules=today_schedules, today=today, show_recent=False)


@main.route('/calendar')
@require_auth
def calendar():
    return render_template('calendar.html')

@main.route('/api/events')
@require_auth
def get_events():
    start_date = request.args.get('start')
    end_date = request.args.get('end')
    
    # å¦‚æœæ²’æœ‰æä¾›æ—¥æœŸåƒæ•¸ï¼Œä½¿ç”¨é è¨­ç¯„åœ
    if not start_date or not end_date:
        start_date = '2024-07-01'
        end_date = '2024-07-31'
    
    try:
        schedules = Schedule.query.filter(
            Schedule.date >= datetime.strptime(start_date, '%Y-%m-%d').date(),
            Schedule.date <= datetime.strptime(end_date, '%Y-%m-%d').date()
        ).all()
        
        events = []
        
        # æŒ‰æ—¥æœŸåˆ†çµ„äº‹ä»¶ï¼Œé¿å…å¤ªå¤šé‡ç–Š
        from collections import defaultdict
        daily_events = defaultdict(list)
        
        for schedule in schedules:
            daily_events[schedule.date].append(schedule)
        
        # ç‚ºæ¯ä¸€å¤©å‰µå»ºäº‹ä»¶
        for schedule_date, day_schedules in daily_events.items():
            if len(day_schedules) <= 3:
                # å¦‚æœç•¶å¤©æ’ç­äººæ•¸å°‘ï¼Œé¡¯ç¤ºå€‹åˆ¥äº‹ä»¶
                for schedule in day_schedules:
                    events.append({
                        'id': f"single_{schedule.id}",
                        'title': f"{schedule.employee.name}({schedule.shift_type.code})",
                        'start': schedule_date.isoformat(),
                        'backgroundColor': schedule.shift_type.color,
                        'borderColor': schedule.shift_type.color,
                        'textColor': '#fff',
                        'extendedProps': {
                            'employee': schedule.employee.name,
                            'shift': schedule.shift_type.name,
                            'shift_code': schedule.shift_type.code,
                            'type': 'single'
                        }
                    })
            else:
                # å¦‚æœç•¶å¤©æ’ç­äººæ•¸å¤šï¼Œé¡¯ç¤ºå½™ç¸½äº‹ä»¶
                shift_summary = {}
                for schedule in day_schedules:
                    shift_code = schedule.shift_type.code
                    if shift_code not in shift_summary:
                        shift_summary[shift_code] = {
                            'count': 0,
                            'color': schedule.shift_type.color,
                            'employees': []
                        }
                    shift_summary[shift_code]['count'] += 1
                    shift_summary[shift_code]['employees'].append(schedule.employee.name)
                
                # ç‚ºæ¯å€‹ç­åˆ¥å‰µå»ºä¸€å€‹äº‹ä»¶
                for shift_code, info in shift_summary.items():
                    events.append({
                        'id': f"summary_{schedule_date}_{shift_code}",
                        'title': f"{shift_code}ç­ ({info['count']}äºº)",
                        'start': schedule_date.isoformat(),
                        'backgroundColor': info['color'],
                        'borderColor': info['color'],
                        'textColor': '#fff',
                        'extendedProps': {
                            'employees': info['employees'],
                            'shift_code': shift_code,
                            'count': info['count'],
                            'type': 'summary',
                            'date': schedule_date.isoformat()
                        }
                    })
        
        return jsonify(events)
        
    except Exception as e:
        print(f"Events API éŒ¯èª¤: {e}")
        return jsonify([]), 500

@main.route('/query_shift')
@require_auth
def query_shift():
    query_date = request.args.get('date')
    schedules = []
    
    # æŒ‡å®šçš„å“¡å·¥åå–®ï¼ˆåŒ…å«ç©ºæ ¼æ ¼å¼ï¼‰
    target_employees = [
        'è³´ ç§‰ å®', 'æ æƒŸ ç¶±', 'æ å®¶ ç‘‹', 'ç‹ å¿— å¿ ', 'é¡§ è‚² ç¦', 
        'èƒ¡ ç¿Š æ½”', 'æœ± å®¶ å¾·', 'é™³ éŸ‹ å¦‚', 'è‘› ç¦', 'äº• åº· ç¾½', 
        'ç°¡ èŠ³ ç‘œ', 'æ¢ å¼˜ å²³', 'æ ä½© ç’‡', 'é„­ æ ¢ ç”', 'ç‹ æ–‡ æ€¡'
    ]
    
    if query_date:
        try:
            target_date = datetime.strptime(query_date, '%Y-%m-%d').date()
            schedules = Schedule.query.filter(Schedule.date == target_date).all()
        except ValueError:
            query_date = None
    
    return render_template('query.html', query_date=query_date, schedules=schedules)

@main.route('/api/query_shift')
@require_auth
def api_query_shift():
    query_date = request.args.get('date')
    
    if query_date:
        target_date = datetime.strptime(query_date, '%Y-%m-%d').date()
        schedules = Schedule.query.filter(Schedule.date == target_date).all()
        
        result = []
        for schedule in schedules:
            result.append({
                'employee': schedule.employee.name,
                'shift': schedule.shift_type.name,
                'shift_code': schedule.shift_type.code,
                'color': schedule.shift_type.color
            })
        
        return jsonify(result)
    
    return jsonify([])




@main.route('/api/shift_types')
def api_shift_types():
    shift_types = ShiftType.query.all()
    result = [{'code': st.code, 'name': st.name, 'color': st.color} for st in shift_types]
    return jsonify(result)

@main.route('/api/export_ics')
def api_export_ics():
    """åŒ¯å‡ºæŒ‡å®šå“¡å·¥çš„æœˆåº¦ç­è¡¨ç‚ºICSæ—¥æ›†æª”æ¡ˆ"""
    try:
        # å–å¾—æŸ¥è©¢åƒæ•¸
        search_query = request.args.get('query', '').strip()
        year = request.args.get('year', type=int)
        month = request.args.get('month', type=int)
        
        if not search_query:
            return jsonify({'error': 'è«‹è¼¸å…¥å“¡å·¥å§“åæˆ–å·¥è™Ÿ'}), 400
            
        if not year or not month:
            return jsonify({'error': 'è«‹é¸æ“‡å¹´æœˆ'}), 400
            
        # ç¢ºä¿æ¸¬è©¦å“¡å·¥å­˜åœ¨ï¼ˆèˆ‡preview_scheduleç›¸åŒé‚è¼¯ï¼‰
        from app.models import db
        employee_data = [
            ('è³´ ç§‰ å®', '8652'),
            ('æ æƒŸ ç¶±', '8312'), 
            ('æ å®¶ ç‘‹', '8512'),
            ('ç‹ å¿— å¿ ', '0450'),
            ('é¡§ è‚² ç¦', '8672'),
            ('èƒ¡ ç¿Š æ½”', '8619'),
            ('æœ± å®¶ å¾·', '8835')
        ]
        
        # æ¯æ¬¡è«‹æ±‚æ™‚ç¢ºä¿å“¡å·¥å­˜åœ¨ï¼ˆè¨˜æ†¶é«”è³‡æ–™åº«éœ€è¦ï¼‰
        for name, code in employee_data:
            employee = Employee.query.filter_by(name=name).first()
            if not employee:
                employee = Employee(name=name, employee_code=code)
                db.session.add(employee)
        db.session.commit()
        
        # æœå°‹å“¡å·¥ï¼ˆèˆ‡preview_scheduleç›¸åŒé‚è¼¯ï¼‰
        employee = Employee.query.filter(
            Employee.name.like(f'%{search_query}%')
        ).first()
        
        # å¦‚æœæ¨¡ç³Šæœå°‹æ‰¾ä¸åˆ°ï¼Œå˜—è©¦ç²¾ç¢ºåŒ¹é…
        if not employee:
            employee = Employee.query.filter_by(name=search_query).first()
        
        # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå˜—è©¦ç§»é™¤ç©ºæ ¼çš„åŒ¹é…
        if not employee:
            search_no_space = search_query.replace(' ', '')
            for emp in Employee.query.all():
                if emp.name.replace(' ', '') == search_no_space:
                    employee = emp
                    break
        
        if not employee:
            return jsonify({'error': f'æ‰¾ä¸åˆ°å“¡å·¥: {search_query}'}), 404
            
        # å–å¾—è©²å“¡å·¥çš„æœˆåº¦æ’ç­è³‡æ–™
        from datetime import date, datetime, time
        import calendar
        
        start_date = date(year, month, 1)
        last_day = calendar.monthrange(year, month)[1]
        end_date = date(year, month, last_day)
        
        schedules = Schedule.query.filter(
            Schedule.employee_id == employee.id,
            Schedule.date >= start_date,
            Schedule.date <= end_date
        ).order_by(Schedule.date.asc()).all()
        
        # ç”ŸæˆICSå…§å®¹
        ics_content = generate_ics_content(employee, schedules, year, month)
        
        # å‰µå»ºéŸ¿æ‡‰ï¼Œé¿å…ä¸­æ–‡æª”åç·¨ç¢¼å•é¡Œ
        import urllib.parse
        filename = f"{employee.name}_{year}å¹´{month:02d}æœˆ_ç­è¡¨.ics"
        encoded_filename = urllib.parse.quote(filename.encode('utf-8'))
        
        response = Response(
            ics_content,
            mimetype='text/calendar',
            headers={
                'Content-Disposition': f'attachment; filename*=UTF-8\'\'{encoded_filename}'
            }
        )
        
        return response
        
    except Exception as e:
        print(f'ICSåŒ¯å‡ºéŒ¯èª¤: {e}')
        return jsonify({'error': f'åŒ¯å‡ºå¤±æ•—: {str(e)}'}), 500

def generate_ics_content(employee, schedules, year, month):
    """æ ¹æ“šGoogleæ—¥æ›†è¦ç¯„ç”ŸæˆICSæª”æ¡ˆå…§å®¹"""
    from datetime import datetime, time
    
    # æŒ‰Googleæ—¥æ›†è¦ç¯„è¨­å®šICSæª”æ¡ˆæ¨™é ­
    ics_lines = [
        'BEGIN:VCALENDAR',
        'VERSION:2.0',
        'PRODID:-//My Calendar Generator//EN',
        'CALSCALE:GREGORIAN',
        'METHOD:PUBLISH',
        f'X-WR-CALNAME:{employee.name}çš„ç­è¡¨',
        'X-WR-TIMEZONE:Asia/Taipei'
    ]
    
    # è™•ç†æ‰€æœ‰ç­åˆ¥ï¼ŒåŒ…å«ä¼‘å‡ç­åˆ¥
    for schedule in schedules:
        
        # å–å¾—ç­åˆ¥çš„å¯¦éš›å·¥ä½œæ™‚é–“
        actual_start = schedule.shift_type.start_time or time(9, 0)
        actual_end = schedule.shift_type.end_time or time(18, 0)
        
        # æŒ‰Googleæ—¥æ›†è¦ç¯„ï¼šæ‰€æœ‰äº‹ä»¶çµ±ä¸€è¨­ç‚º09:00-09:05
        event_start = datetime.combine(schedule.date, time(9, 0))
        event_end = datetime.combine(schedule.date, time(9, 5))
        
        # æ ¼å¼åŒ–ç‚ºICSæ™‚é–“æ ¼å¼
        dtstart = event_start.strftime('%Y%m%dT%H%M%S')
        dtend = event_end.strftime('%Y%m%dT%H%M%S')
        
        # å»ºç«‹ç­åˆ¥æ¨™é¡Œï¼Œåªé¡¯ç¤ºç­åˆ¥ä»£ç¢¼
        summary = f'{schedule.shift_type.code}'
        
        # æŒ‰ç…§Googleæ—¥æ›†è¦ç¯„çš„ç°¡åŒ–æ ¼å¼å‰µå»ºäº‹ä»¶
        event_lines = [
            'BEGIN:VEVENT',
            f'DTSTART:{dtstart}',
            f'DTEND:{dtend}',
            f'SUMMARY:{summary}',
            'END:VEVENT'
        ]
        
        ics_lines.extend(event_lines)
    
    # ICSæª”æ¡ˆçµå°¾
    ics_lines.append('END:VCALENDAR')
    
    return '\r\n'.join(ics_lines)

@main.route('/api/date_range')
def api_date_range():
    """ç²å–è³‡æ–™åº«ä¸­å¯¦éš›çš„æ—¥æœŸç¯„åœï¼Œæ”¯æ´å¤šæœˆä»½"""
    first_schedule = Schedule.query.order_by(Schedule.date.asc()).first()
    last_schedule = Schedule.query.order_by(Schedule.date.desc()).first()
    
    if first_schedule and last_schedule:
        # ç²å–æ‰€æœ‰æœ‰è³‡æ–™çš„æ—¥æœŸ
        all_dates = [s.date.strftime('%Y-%m-%d') for s in Schedule.query.all()]
        unique_dates = sorted(list(set(all_dates)))
        
        # çµ±è¨ˆå„æœˆä»½çš„è³‡æ–™
        month_stats = {}
        for schedule in Schedule.query.all():
            year_month = f'{schedule.date.year}-{schedule.date.month:02d}'
            if year_month not in month_stats:
                month_stats[year_month] = {
                    'count': 0,
                    'year': schedule.date.year,
                    'month': schedule.date.month,
                    'display': f'{schedule.date.year}å¹´{schedule.date.month:02d}æœˆ'
                }
            month_stats[year_month]['count'] += 1
        
        # å¦‚æœè·¨æœˆä»½ï¼Œé¡¯ç¤ºç¯„åœï¼›å¦‚æœåŒæœˆä»½ï¼Œé¡¯ç¤ºå–®æœˆ
        if first_schedule.date.month == last_schedule.date.month and first_schedule.date.year == last_schedule.date.year:
            month_display = first_schedule.date.strftime('%Yå¹´%mæœˆ')
        else:
            month_display = f'{first_schedule.date.strftime("%Yå¹´%mæœˆ")} è‡³ {last_schedule.date.strftime("%Yå¹´%mæœˆ")}'
        
        return jsonify({
            'start_date': first_schedule.date.strftime('%Y-%m-%d'),
            'end_date': last_schedule.date.strftime('%Y-%m-%d'),
            'month_year': month_display,
            'year': first_schedule.date.year,
            'month': first_schedule.date.month,
            'available_dates': unique_dates,
            'available_months': month_stats
        })
    else:
        return jsonify({
            'start_date': '2025-07-01',
            'end_date': '2025-07-31',
            'month_year': '2025å¹´07æœˆ',
            'year': 2025,
            'month': 7,
            'available_dates': [],
            'available_months': {}
        })

@main.route('/export_schedule')
@require_auth
def export_schedule():
    """ä¸€éµåŒ¯å‡ºæœˆåº¦ç­è¡¨é é¢"""
    return render_template('export.html')

@main.route('/api/preview_schedule')
def api_preview_schedule():
    """æä¾›ç­è¡¨é è¦½è³‡æ–™çµ¦å‰ç«¯ç”ŸæˆJPG"""
    try:
        # å–å¾—æŸ¥è©¢åƒæ•¸
        search_query = request.args.get('query', '').strip()
        year = request.args.get('year', type=int)
        month = request.args.get('month', type=int)
        
        if not search_query:
            return jsonify({'error': 'è«‹è¼¸å…¥å“¡å·¥å§“åæˆ–å·¥è™Ÿ'}), 400
            
        if not year or not month:
            return jsonify({'error': 'è«‹é¸æ“‡å¹´æœˆ'}), 400
            
        # ç¢ºä¿æ¸¬è©¦å“¡å·¥å­˜åœ¨
        from app.models import db
        employee_data = [
            ('è³´ ç§‰ å®', '8652'),
            ('æ æƒŸ ç¶±', '8312'), 
            ('æ å®¶ ç‘‹', '8512'),
            ('ç‹ å¿— å¿ ', '0450'),
            ('é¡§ è‚² ç¦', '8672'),
            ('èƒ¡ ç¿Š æ½”', '8619'),
            ('æœ± å®¶ å¾·', '8835')
        ]
        
        # æ¯æ¬¡è«‹æ±‚æ™‚ç¢ºä¿å“¡å·¥å­˜åœ¨ï¼ˆè¨˜æ†¶é«”è³‡æ–™åº«éœ€è¦ï¼‰
        for name, code in employee_data:
            employee = Employee.query.filter_by(name=name).first()
            if not employee:
                employee = Employee(name=name, employee_code=code)
                db.session.add(employee)
        db.session.commit()
        
        # å¢å¼·çš„å“¡å·¥æœå°‹é‚è¼¯ï¼ˆæ”¯æ´å¤šç¨®æ¨¡ç³ŠåŒ¹é…ï¼‰
        employee = None
        
        # 1. å…ˆå˜—è©¦å“¡å·¥ä»£ç¢¼å®Œå…¨åŒ¹é…
        employee = Employee.query.filter_by(employee_code=search_query).first()
        
        # 2. å˜—è©¦å“¡å·¥ä»£ç¢¼éƒ¨åˆ†åŒ¹é…
        if not employee:
            employee = Employee.query.filter(
                Employee.employee_code.like(f'%{search_query}%')
            ).first()
        
        # 3. å˜—è©¦å§“åå®Œå…¨åŒ¹é…
        if not employee:
            employee = Employee.query.filter_by(name=search_query).first()
        
        # 4. å°æ–¼çŸ­æŸ¥è©¢å­—ä¸²ï¼ˆ1-2å€‹å­—ï¼‰ï¼Œç›´æ¥é€²å…¥å¤šå“¡å·¥åŒ¹é…é‚è¼¯
        if not employee and len(search_query.replace(' ', '')) <= 2:
            search_clean = search_query.replace(' ', '')
            matching_employees = []
            for emp in Employee.query.all():
                emp_name_clean = emp.name.replace(' ', '')
                # æ”¯æ´è¼¸å…¥ã€Œæã€æ‰¾åˆ°æ‰€æœ‰å§“æçš„å“¡å·¥
                if search_clean in emp_name_clean:
                    matching_employees.append(emp)
            
            # å¦‚æœæ‰¾åˆ°å¤šå€‹åŒ¹é…å“¡å·¥ï¼Œè¿”å›é¸æ“‡åˆ—è¡¨è®“ç”¨æˆ¶é¸æ“‡
            if len(matching_employees) > 1:
                matching_employees.sort(key=lambda e: e.employee_code)
                employee_choices = []
                for emp in matching_employees:
                    employee_choices.append({
                        'employee_code': emp.employee_code,
                        'name': emp.name,
                        'id': emp.id
                    })
                return jsonify({
                    'multiple_matches': True,
                    'choices': employee_choices,
                    'query': search_query
                })
            elif len(matching_employees) == 1:
                employee = matching_employees[0]
        
        # 5. å˜—è©¦å§“åæ¨¡ç³Šæœå°‹ï¼ˆåŒ…å«æŸ¥è©¢å­—ä¸²ï¼‰- åƒ…é™æ–¼è¼ƒé•·çš„æŸ¥è©¢å­—ä¸²
        if not employee and len(search_query.replace(' ', '')) > 2:
            employee = Employee.query.filter(
                Employee.name.like(f'%{search_query}%')
            ).first()
        
        # 6. ç§»é™¤ç©ºæ ¼å¾Œçš„åŒ¹é…ï¼ˆè™•ç†ç©ºæ ¼å•é¡Œï¼‰
        if not employee:
            search_no_space = search_query.replace(' ', '')
            for emp in Employee.query.all():
                # å§“åç§»é™¤ç©ºæ ¼æ¯”å°
                if emp.name.replace(' ', '') == search_no_space:
                    employee = emp
                    break
                # å“¡å·¥ä»£ç¢¼ç§»é™¤ç©ºæ ¼æ¯”å°
                if emp.employee_code.replace(' ', '') == search_no_space:
                    employee = emp
                    break
        
        if not employee:
            return jsonify({'error': f'æ‰¾ä¸åˆ°å“¡å·¥: {search_query}'}), 404
            
        # å–å¾—è©²å“¡å·¥çš„æœˆåº¦æ’ç­è³‡æ–™
        from datetime import date
        import calendar
        
        start_date = date(year, month, 1)
        last_day = calendar.monthrange(year, month)[1]
        end_date = date(year, month, last_day)
        
        schedules = Schedule.query.filter(
            Schedule.employee_id == employee.id,
            Schedule.date >= start_date,
            Schedule.date <= end_date
        ).order_by(Schedule.date.asc()).all()
        
        # å…è¨±é¡¯ç¤ºéƒ¨åˆ†è³‡æ–™æˆ–ç©ºç™½æœˆæ›†ï¼Œä¸å†è¦æ±‚å¿…é ˆæœ‰å®Œæ•´è³‡æ–™
        # if not schedules:
        #     return jsonify({'error': f'{employee.name} åœ¨ {year}å¹´{month:02d}æœˆ æ²’æœ‰æ’ç­è³‡æ–™'}), 404
            
        # å»ºç«‹æœˆæ›†çµæ§‹ (ä½¿ç”¨æ˜ŸæœŸæ—¥é–‹å§‹çš„å‚³çµ±æ ¼å¼)
        calendar.setfirstweekday(calendar.SUNDAY)
        cal = calendar.monthcalendar(year, month)
        calendar.setfirstweekday(calendar.MONDAY)  # æ¢å¾©é è¨­
        
        # æº–å‚™æ’ç­è³‡æ–™
        schedule_data = []
        for schedule in schedules:
            schedule_data.append({
                'date': schedule.date.isoformat(),
                'shift_code': schedule.shift_type.code,
                'shift_name': schedule.shift_type.name
            })
        
        # æº–å‚™å›å‚³è³‡æ–™
        response_data = {
            'employee': {
                'name': employee.name,
                'employee_code': employee.employee_code
            },
            'year': year,
            'month': month,
            'schedules': schedule_data,
            'calendar': cal
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        print(f'é è¦½APIéŒ¯èª¤: {e}')
        return jsonify({'error': str(e)}), 500

@main.route('/api/preview_schedule_by_id')
def api_preview_schedule_by_id():
    """æ ¹æ“šå“¡å·¥IDç›´æ¥é è¦½ç­è¡¨ï¼ˆç”¨æ–¼ç”¨æˆ¶é¸æ“‡ç‰¹å®šå“¡å·¥å¾Œï¼‰"""
    try:
        employee_id = request.args.get('employee_id')
        year = int(request.args.get('year'))
        month = int(request.args.get('month'))
        
        if not employee_id:
            return jsonify({'error': 'ç¼ºå°‘å“¡å·¥ID'}), 400
        
        # ç›´æ¥æ ¹æ“šIDæŸ¥æ‰¾å“¡å·¥
        employee = Employee.query.get(employee_id)
        if not employee:
            return jsonify({'error': f'æ‰¾ä¸åˆ°å“¡å·¥ID: {employee_id}'}), 404
        
        # å–å¾—è©²å“¡å·¥çš„æœˆåº¦æ’ç­è³‡æ–™
        from datetime import date
        import calendar
        
        # ç”ŸæˆæŒ‡å®šæœˆä»½çš„æ‰€æœ‰æ—¥æœŸ
        year_month = date(year, month, 1)
        days_in_month = calendar.monthrange(year, month)[1]
        
        schedules = Schedule.query.filter(
            Schedule.employee_id == employee.id,
            db.extract('year', Schedule.date) == year,
            db.extract('month', Schedule.date) == month
        ).all()
        
        # å°‡æ’ç­è³‡æ–™è½‰æ›ç‚ºå­—å…¸æ ¼å¼ï¼Œæ–¹ä¾¿å‰ç«¯è™•ç†
        schedule_dict = {schedule.date.day: schedule for schedule in schedules}
        
        # ç”Ÿæˆæœˆæ›†çµæ§‹ (ä½¿ç”¨æ˜ŸæœŸæ—¥é–‹å§‹çš„å‚³çµ±æ ¼å¼)
        calendar.setfirstweekday(calendar.SUNDAY)
        cal = calendar.monthcalendar(year, month)
        calendar.setfirstweekday(calendar.MONDAY)  # æ¢å¾©é è¨­
        
        # æº–å‚™è¿”å›çš„æ’ç­è³‡æ–™
        schedule_data = []
        for schedule in schedules:
            # é€éé—œè¯ç²å–ç­åˆ¥é¡å‹è³‡æ–™
            shift_type = schedule.shift_type
            schedule_data.append({
                'date': schedule.date.strftime('%Y-%m-%d'),
                'shift_code': shift_type.code if shift_type else 'UNKNOWN',
                'shift_name': shift_type.name if shift_type else 'Unknown Shift'
            })
        
        return jsonify({
            'employee': {
                'name': employee.name,
                'employee_code': employee.employee_code
            },
            'year': year,
            'month': month,
            'calendar': cal,
            'schedules': schedule_data
        })
        
    except Exception as e:
        print(f'æŒ‰IDé è¦½APIéŒ¯èª¤: {e}')
        return jsonify({'error': str(e)}), 500

@main.route('/api/export_monthly_schedule')
def api_export_monthly_schedule():
    """åŒ¯å‡ºæŒ‡å®šå“¡å·¥çš„æœˆåº¦ç­è¡¨ç‚ºExcel"""
    try:
        # å–å¾—æŸ¥è©¢åƒæ•¸
        search_query = request.args.get('query', '').strip()
        year = request.args.get('year', type=int)
        month = request.args.get('month', type=int)
        
        if not search_query:
            return jsonify({'error': 'è«‹è¼¸å…¥å“¡å·¥å§“åæˆ–å·¥è™Ÿ'}), 400
            
        if not year or not month:
            return jsonify({'error': 'è«‹é¸æ“‡å¹´æœˆ'}), 400
            
        # æœå°‹å“¡å·¥ï¼ˆæ”¯æ´å§“åæˆ–å“¡å·¥ä»£ç¢¼ï¼‰
        employee = Employee.query.filter(
            (Employee.name.like(f'%{search_query}%')) |
            (Employee.employee_code.like(f'%{search_query}%'))
        ).first()
        
        if not employee:
            return jsonify({'error': f'æ‰¾ä¸åˆ°å“¡å·¥: {search_query}'}), 404
            
        # å–å¾—è©²å“¡å·¥çš„æœˆåº¦æ’ç­è³‡æ–™
        from datetime import date
        start_date = date(year, month, 1)
        
        # è¨ˆç®—æœˆä»½çš„æœ€å¾Œä¸€å¤©
        import calendar
        last_day = calendar.monthrange(year, month)[1]
        end_date = date(year, month, last_day)
        
        schedules = Schedule.query.filter(
            Schedule.employee_id == employee.id,
            Schedule.date >= start_date,
            Schedule.date <= end_date
        ).order_by(Schedule.date.asc()).all()
        
        if not schedules:
            return jsonify({'error': f'{employee.name} åœ¨ {year}å¹´{month:02d}æœˆ æ²’æœ‰æ’ç­è³‡æ–™'}), 404
            
        # å»ºç«‹æœˆæ›†å¼ExcelåŒ¯å‡º
        import pandas as pd
        from io import BytesIO
        import calendar
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Border, Side, Alignment
        
        # å»ºç«‹å·¥ä½œç°¿
        wb = Workbook()
        ws = wb.active
        ws.title = 'æœˆåº¦ç­è¡¨'
        
        # è¨­å®šæ¨™é¡Œ
        ws['A1'] = f'{employee.name} å€‹äººç­è¡¨'
        ws['A2'] = f'å·¥è™Ÿ: {employee.employee_code}'
        ws['A3'] = f'{year}å¹´{month:02d}æœˆ'
        
        # åˆä½µæ¨™é¡Œå„²å­˜æ ¼
        ws.merge_cells('A1:H1')
        ws.merge_cells('A2:H2')
        ws.merge_cells('A3:H3')
        
        # è¨­å®šæ¨™é¡Œæ¨£å¼
        title_font = Font(name='å¾®è»Ÿæ­£é»‘é«”', size=16, bold=True)
        subtitle_font = Font(name='å¾®è»Ÿæ­£é»‘é«”', size=12)
        ws['A1'].font = title_font
        ws['A2'].font = subtitle_font
        ws['A3'].font = subtitle_font
        ws['A1'].alignment = Alignment(horizontal='center')
        ws['A2'].alignment = Alignment(horizontal='center')
        ws['A3'].alignment = Alignment(horizontal='center')
        
        # å»ºç«‹ç­è¡¨è³‡æ–™å­—å…¸
        schedule_dict = {}
        for schedule in schedules:
            schedule_dict[schedule.date.day] = schedule.shift_type.code
        
        # å–å¾—è©²æœˆçš„æ—¥æ›†è³‡è¨Š (ä½¿ç”¨æ˜ŸæœŸæ—¥é–‹å§‹çš„å‚³çµ±æ ¼å¼)
        calendar.setfirstweekday(calendar.SUNDAY)
        cal = calendar.monthcalendar(year, month)
        calendar.setfirstweekday(calendar.MONDAY)  # æ¢å¾©é è¨­
        weekdays = ['æ˜ŸæœŸæ—¥', 'æ˜ŸæœŸä¸€', 'æ˜ŸæœŸäºŒ', 'æ˜ŸæœŸä¸‰', 'æ˜ŸæœŸå››', 'æ˜ŸæœŸäº”', 'æ˜ŸæœŸå…­']
        
        # å¾ç¬¬5è¡Œé–‹å§‹å»ºç«‹æ—¥æ›†è¡¨æ ¼
        start_row = 5
        
        # å¯«å…¥æ˜ŸæœŸæ¨™é¡Œ
        for col, weekday in enumerate(weekdays, 1):
            cell = ws.cell(row=start_row, column=col, value=weekday)
            cell.font = Font(name='å¾®è»Ÿæ­£é»‘é«”', size=12, bold=True)
            cell.fill = PatternFill(start_color='D9E2F3', end_color='D9E2F3', fill_type='solid')
            cell.alignment = Alignment(horizontal='center')
            
        # è¨­å®šé‚Šæ¡†æ¨£å¼
        thin_border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )
        
        # å¡«å…¥æ—¥æœŸå’Œç­åˆ¥
        for week_num, week in enumerate(cal):
            row = start_row + 1 + week_num
            for day_num, day in enumerate(week, 1):
                if day == 0:  # ç©ºç™½æ—¥æœŸ
                    cell = ws.cell(row=row, column=day_num, value='')
                else:
                    # æ—¥æœŸ
                    date_str = str(day)
                    # ç­åˆ¥ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                    shift_code = schedule_dict.get(day, '')
                    
                    if shift_code:
                        cell_value = f'{date_str}\n{shift_code}'
                        # è¨­å®šç­åˆ¥èƒŒæ™¯è‰²
                        if 'H' in shift_code:  # ä¼‘å‡
                            fill_color = 'FFE6CC'  # æ·ºæ©™è‰²
                        elif 'P1' in shift_code:  # P1ç­
                            fill_color = 'E2EFDA'  # æ·ºç¶ è‰²
                        elif 'P2' in shift_code:  # P2ç­
                            fill_color = 'FFF2CC'  # æ·ºé»ƒè‰²
                        elif 'P3' in shift_code:  # P3ç­
                            fill_color = 'FCE4D6'  # æ·ºæ©™è‰²
                        elif 'P4' in shift_code:  # P4ç­
                            fill_color = 'F4CCCC'  # æ·ºç´…è‰²
                        elif 'FC' in shift_code:  # FCç­
                            fill_color = 'D0E0E3'  # æ·ºè—è‰²
                        else:
                            fill_color = 'F2F2F2'  # æ·ºç°è‰²
                        
                        cell = ws.cell(row=row, column=day_num, value=cell_value)
                        cell.fill = PatternFill(start_color=fill_color, end_color=fill_color, fill_type='solid')
                    else:
                        cell = ws.cell(row=row, column=day_num, value=date_str)
                        cell.fill = PatternFill(start_color='F8F9FA', end_color='F8F9FA', fill_type='solid')
                    
                    cell.font = Font(name='å¾®è»Ÿæ­£é»‘é«”', size=10)
                    cell.alignment = Alignment(horizontal='center', vertical='center')
                
                # æ·»åŠ é‚Šæ¡†
                ws.cell(row=row, column=day_num).border = thin_border
        
        # ç‚ºæ˜ŸæœŸæ¨™é¡Œè¡Œä¹Ÿæ·»åŠ é‚Šæ¡†
        for col in range(1, 8):
            ws.cell(row=start_row, column=col).border = thin_border
        
        # è¨­å®šæ¬„ä½å¯¬åº¦
        for col in range(1, 8):
            ws.column_dimensions[chr(64 + col)].width = 12
        
        # è¨­å®šè¡Œé«˜
        for row in range(start_row + 1, start_row + 1 + len(cal)):
            ws.row_dimensions[row].height = 40
        
        # æ·»åŠ åœ–ä¾‹èªªæ˜
        legend_start_row = start_row + len(cal) + 2
        ws.cell(row=legend_start_row, column=1, value='ç­åˆ¥èªªæ˜:').font = Font(name='å¾®è»Ÿæ­£é»‘é«”', size=12, bold=True)
        
        # å–å¾—è©²å“¡å·¥ä½¿ç”¨çš„ç­åˆ¥é¡å‹
        used_shift_types = set()
        for schedule in schedules:
            used_shift_types.add((schedule.shift_type.code, schedule.shift_type.color))
        
        legend_row = legend_start_row + 1
        for i, (code, color) in enumerate(sorted(used_shift_types)):
            legend_cell = ws.cell(row=legend_row + i, column=1, value=f'{code}: {code}')
            legend_cell.font = Font(name='å¾®è»Ÿæ­£é»‘é«”', size=10)
            # è¨­å®šèƒŒæ™¯è‰²èˆ‡ç­åˆ¥é¡è‰²ç›¸åŒ
            if color and color.startswith('#'):
                # ç§»é™¤#è™Ÿä¸¦è½‰ç‚ºå¤§å¯«
                hex_color = color[1:].upper()
                legend_cell.fill = PatternFill(start_color=hex_color, end_color=hex_color, fill_type='solid')
        
        # ä¿å­˜åˆ°BytesIO
        output = BytesIO()
        wb.save(output)
            
        output.seek(0)
        
        # æº–å‚™æª”æ¡ˆåç¨±
        filename = f'{employee.name}_{year}å¹´{month:02d}æœˆ_ç­è¡¨.xlsx'
        
        # è¿”å›æª”æ¡ˆä¸‹è¼‰
        from flask import send_file
        return send_file(
            output,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            as_attachment=True,
            download_name=filename
        )
        
    except Exception as e:
        print(f'åŒ¯å‡ºéŒ¯èª¤: {e}')
        return jsonify({'error': str(e)}), 500

@main.route('/upload_excel', methods=['GET', 'POST'])
@require_admin
def upload_excel():
    if request.method == 'POST':
        try:
            if 'file' not in request.files:
                return render_template('upload.html', error='è«‹é¸æ“‡æª”æ¡ˆ')
            
            file = request.files['file']
            if file.filename == '':
                return render_template('upload.html', error='è«‹é¸æ“‡æª”æ¡ˆ')
            
            if not file.filename.endswith('.csv'):
                return render_template('upload.html', error='åªæ”¯æ´ CSV æ ¼å¼æª”æ¡ˆ')
            
            print(f'é–‹å§‹è™•ç†æª”æ¡ˆ: {file.filename}')
            
            # å˜—è©¦ä¸åŒçš„æ–¹æ³•è®€å– Excel æª”æ¡ˆ
            df = None
            for header_row in [0, 1, 2, 3, 4, 5]:  # å˜—è©¦å‰6è¡Œä½œç‚ºæ¨™é¡Œè¡Œ
                try:
                    temp_df = pd.read_excel(file, header=header_row)
                    print(f'å˜—è©¦æ¨™é¡Œè¡Œ {header_row}: {list(temp_df.columns)[:10]}')  # åªé¡¯ç¤ºå‰10å€‹æ¬„ä½
                    
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«æˆ‘å€‘éœ€è¦çš„æ¬„ä½
                    columns_str = '|'.join(str(col).lower() for col in temp_df.columns)
                    if any(keyword in columns_str for keyword in ['å§“å', 'å“¡å·¥', 'æ—¥æœŸ', 'ç­', 'æ™‚é–“']):
                        df = temp_df
                        print(f'âœ… åœ¨ç¬¬ {header_row} è¡Œæ‰¾åˆ°æœ‰æ•ˆæ¨™é¡Œ')
                        break
                except Exception as e:
                    print(f'æ¨™é¡Œè¡Œ {header_row} è®€å–å¤±æ•—: {e}')
                    continue
            
            if df is None:
                # å¦‚æœæ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆæ¨™é¡Œï¼Œä½¿ç”¨ç¬¬ä¸€è¡Œ
                df = pd.read_excel(file)
                print('âš ï¸ æœªæ‰¾åˆ°æ˜ç¢ºæ¨™é¡Œè¡Œï¼Œä½¿ç”¨é è¨­æ ¼å¼')
            
            print(f'æœ€çµ‚è®€å–åˆ° {len(df)} è¡Œæ•¸æ“š')
            print(f'æœ€çµ‚æ¬„ä½: {list(df.columns)}')
            
            processed_count = 0
            error_count = 0
            
            # åµæ¸¬æ©«å‘ç­è¡¨æ ¼å¼
            for index, row in df.iterrows():
                try:
                    # å°‹æ‰¾å“¡å·¥å§“åï¼ˆé€šå¸¸åœ¨ç¬¬3æˆ–ç¬¬4æ¬„ï¼‰
                    employee_name = None
                    for col_idx in [3, 2, 1, 0]:  # æª¢æŸ¥å¯èƒ½çš„å§“åæ¬„ä½
                        col_name = f'Unnamed: {col_idx}' if col_idx > 0 else 'Unnamed: 0'
                        if col_name in row and not pd.isna(row[col_name]):
                            name_value = str(row[col_name]).strip()
                            # æª¢æŸ¥æ˜¯å¦çœ‹èµ·ä¾†åƒå§“åï¼ˆåŒ…å«ä¸­æ–‡å­—ä¸”ä¸æ˜¯æ•¸å­—æˆ–ç­åˆ¥ä»£ç¢¼ï¼‰
                            if name_value and not name_value.isdigit() and len(name_value) >= 2 and 'ï¼ˆç•°å‹•å¾Œï¼‰' not in name_value:
                                # é€²ä¸€æ­¥æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆå§“åæ ¼å¼
                                if any('\u4e00' <= char <= '\u9fff' for char in name_value):  # åŒ…å«ä¸­æ–‡
                                    employee_name = name_value
                                    print(f'æ‰¾åˆ°å“¡å·¥å§“å: {employee_name} (ç¬¬ {index+1} è¡Œ)')
                                    break
                    
                    if not employee_name:
                        continue
                    
                    # è™•ç†æ©«å‘ç­è¡¨ï¼šæ¯å€‹æ¬„ä½ä»£è¡¨ä¸€å¤©çš„ç­åˆ¥
                    current_date = datetime(2025, 7, 1).date()  # 114å¹´7æœˆ = 2025å¹´7æœˆ
                    
                    for col_name, shift_value in row.items():
                        if col_name.startswith('Unnamed:') and not pd.isna(shift_value):
                            shift_code = str(shift_value).strip()
                            print(f'  æª¢æŸ¥æ¬„ä½ {col_name}: å€¼={shift_value} -> ç­åˆ¥ä»£ç¢¼={shift_code}')
                            
                            # éæ¿¾æ‰éç­åˆ¥ä»£ç¢¼çš„å€¼
                            if shift_code and shift_code not in ['nan', employee_name] and len(shift_code) <= 10:
                                # æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ç­åˆ¥ä»£ç¢¼ - æ”¾å¯¬æ¢ä»¶
                                print(f'  æª¢æŸ¥ç­åˆ¥ä»£ç¢¼ {shift_code} æ˜¯å¦æœ‰æ•ˆ...')
                                # æ›´å¯¬é¬†çš„ç­åˆ¥ä»£ç¢¼é©—è­‰ï¼šåªè¦ä¸æ˜¯ç´”æ•¸å­—ä¸”é•·åº¦åˆç†
                                if (not shift_code.isdigit() and 
                                    len(shift_code) >= 1 and 
                                    shift_code not in ['nan', 'NaN', 'None', ''] and
                                    not shift_code.replace('.', '').isdigit()):
                                    print(f'  âœ… æ‰¾åˆ°æœ‰æ•ˆç­åˆ¥: {shift_code}')
                                    
                                    # æŸ¥è©¢ç¾æœ‰å“¡å·¥è¨˜éŒ„
                                    existing_employee = Employee.query.filter_by(name=employee_name).first()
                                    if not existing_employee:
                                        # ç”Ÿæˆå”¯ä¸€çš„å“¡å·¥ä»£ç¢¼
                                        existing_count = Employee.query.count()
                                        employee_code = f"EMP_{existing_count+1:03d}"
                                        
                                        # ç¢ºä¿ä»£ç¢¼å”¯ä¸€æ€§
                                        while Employee.query.filter_by(employee_code=employee_code).first():
                                            existing_count += 1
                                            employee_code = f"EMP_{existing_count+1:03d}"
                                        
                                        new_employee = Employee(name=employee_name, employee_code=employee_code)
                                        db.session.add(new_employee)
                                        try:
                                            db.session.commit()
                                        except Exception as e:
                                            db.session.rollback()
                                            existing_employee = Employee.query.filter_by(name=employee_name).first()
                                            if not existing_employee:
                                                raise e
                                    else:
                                        new_employee = existing_employee
                                    
                                    employee = new_employee if 'new_employee' in locals() else existing_employee
                                    
                                    # è™•ç†ç­åˆ¥é¡å‹
                                    shift_type = ShiftType.query.filter_by(code=shift_code).first()
                                    if not shift_type:
                                        shift_name, start_time, end_time, color = get_shift_info(shift_code)
                                        shift_type = ShiftType(
                                            code=shift_code, 
                                            name=shift_name, 
                                            start_time=start_time, 
                                            end_time=end_time,
                                            color=color
                                        )
                                        db.session.add(shift_type)
                                    
                                    # è¨ˆç®—å°æ‡‰çš„æ—¥æœŸï¼ˆä¾æ¬„ä½ä½ç½®æ¨ç®—ï¼‰
                                    col_num = int(col_name.split(': ')[1]) if ': ' in col_name else 0
                                    day_offset = col_num - 4  # å‡è¨­ç¬¬4æ¬„é–‹å§‹æ˜¯7/1
                                    if day_offset >= 0 and day_offset < 31:  # 7æœˆæœ‰31å¤©
                                        schedule_date = datetime(2025, 7, day_offset + 1).date()
                                        
                                        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨è¨˜éŒ„
                                        existing_schedule = Schedule.query.filter_by(
                                            date=schedule_date,
                                            employee=employee
                                        ).first()
                                        
                                        if not existing_schedule:
                                            schedule = Schedule(
                                                date=schedule_date,
                                                employee=employee,
                                                shift_type=shift_type
                                            )
                                            db.session.add(schedule)
                                            processed_count += 1
                                            print(f'è™•ç†: {employee_name} - {schedule_date} - {shift_code}')
                    
                except Exception as row_error:
                    error_count += 1
                    print(f'ç¬¬ {index+1} è¡Œè™•ç†éŒ¯èª¤: {row_error}')
                    continue
            
            db.session.commit()
            success_message = f'åŒ¯å…¥å®Œæˆï¼è™•ç†äº† {processed_count} ç­†è¨˜éŒ„ï¼Œ{error_count} ç­†éŒ¯èª¤'
            print(success_message)
            return render_template('upload.html', success=success_message)
            
        except Exception as e:
            db.session.rollback()
            error_message = f'åŒ¯å…¥å¤±æ•—: {str(e)}'
            print(error_message)
            return render_template('upload.html', error=error_message)
    
    return render_template('upload.html')

def get_shift_info(shift_code):
    """æ ¹æ“šç­åˆ¥ä»£ç¢¼è¿”å›ç­åˆ¥è³‡è¨Šï¼Œæ”¯æ´è¤‡åˆæ ¼å¼å’Œæœªæ’ç­ç‹€æ…‹"""
    from datetime import time
    
    # ç‰¹æ®Šç‹€æ…‹ï¼šæœªæ’ç­
    if shift_code == 'UNASSIGNED':
        return 'æœªæ’ç­', time(0, 0), time(0, 0), '#9ca3af'
    
    # ä¼‘å‡ç›¸é—œ
    if shift_code in ['H0', 'H1', 'H2']:
        return f'ä¼‘å‡({shift_code})', time(0, 0), time(0, 0), '#6c757d'
    
    # æ­£å¸¸ç­åˆ¥
    elif shift_code == 'FC':
        return 'FCç­', time(9, 0), time(18, 0), '#007bff'
    elif shift_code == 'FX':
        return 'FXç­', time(10, 0), time(19, 0), '#495057'
    
    # è™•ç†è¤‡åˆç­åˆ¥ï¼ˆå¦‚ P1c/å·¥ç¨‹, P4p/ME ç­‰ï¼‰
    base_code = shift_code.split('/')[0] if '/' in shift_code else shift_code
    suffix = shift_code.split('/')[1] if '/' in shift_code else ''
    
    # Pç­ç³»åˆ—ï¼ˆæ ¹æ“šåŸºç¤ä»£ç¢¼åˆ¤æ–·æ™‚é–“ï¼‰
    if base_code.startswith('P1'):
        name = f'{shift_code}ç­' if '/' in shift_code else f'P1ç­({shift_code})'
        return name, time(8, 0), time(17, 0), '#28a745'
    elif base_code.startswith('P2'):
        name = f'{shift_code}ç­' if '/' in shift_code else f'P2ç­({shift_code})'
        return name, time(14, 0), time(23, 0), '#ffc107'
    elif base_code.startswith('P3'):
        name = f'{shift_code}ç­' if '/' in shift_code else f'P3ç­({shift_code})'
        return name, time(17, 0), time(2, 0), '#fd7e14'
    elif base_code.startswith('P4'):
        name = f'{shift_code}ç­' if '/' in shift_code else f'P4ç­({shift_code})'
        return name, time(20, 0), time(5, 0), '#dc3545'
    elif base_code.startswith('P5'):
        name = f'{shift_code}ç­' if '/' in shift_code else f'P5ç­({shift_code})'
        return name, time(22, 0), time(7, 0), '#6f42c1'
    
    # å…¶ä»–ç­åˆ¥ç³»åˆ—
    elif base_code.startswith('N'):
        return f'{shift_code}ç­', time(19, 0), time(4, 0), '#20c997'
    elif base_code.startswith('E'):
        return f'{shift_code}ç­', time(7, 0), time(16, 0), '#17a2b8'
    elif base_code.startswith('C'):
        return f'{shift_code}ç­', time(16, 0), time(1, 0), '#e83e8c'
    elif base_code.startswith('R'):
        return f'{shift_code}ç­', time(12, 0), time(21, 0), '#6610f2'
    elif shift_code in ['NT', 'CH']:
        return f'{shift_code}ç­', time(9, 0), time(18, 0), '#343a40'
    
    # ç‰¹æ®Šæ ¼å¼è™•ç†ï¼ˆå¦‚ FC/å‰è£½ï¼‰
    elif base_code == 'FC' and suffix:
        return f'FCç­/{suffix}', time(9, 0), time(18, 0), '#007bff'
    
    # å…¶ä»–æœªçŸ¥ç­åˆ¥ï¼ˆå¼·åˆ¶åŒ¯å…¥æ™‚å¯èƒ½ç”¢ç”Ÿï¼‰
    else:
        return f'{shift_code}ç­', time(9, 0), time(18, 0), '#868e96'

@main.route('/employee/<int:employee_id>/schedule')
def employee_schedule(employee_id):
    employee = Employee.query.get_or_404(employee_id)
    schedules = Schedule.query.filter_by(employee_id=employee_id).order_by(Schedule.date.desc()).all()
    today = date.today()
    
    return render_template('employee_schedule.html', employee=employee, schedules=schedules, today=today)

# ===== ç®¡ç†å“¡åŠŸèƒ½ =====

@main.route('/api/admin/pending-users')
@require_admin
def get_pending_users():
    """ç²å–å¾…å¯©æ ¸ç”¨æˆ¶åˆ—è¡¨"""
    try:
        pending_users = User.query.filter_by(status='pending').order_by(User.created_at.desc()).all()
        
        result = []
        for user in pending_users:
            result.append({
                'id': user.id,
                'username': user.username,
                'name': user.name,
                'status': user.status,
                'created_at': user.created_at.isoformat()
            })
        
        return jsonify({
            'success': True,
            'data': result
        })
        
    except Exception as e:
        print(f'ç²å–å¾…å¯©æ ¸ç”¨æˆ¶éŒ¯èª¤: {e}')
        return jsonify({'success': False, 'message': 'ç²å–ç”¨æˆ¶åˆ—è¡¨å¤±æ•—'}), 500

@main.route('/api/admin/approve-user', methods=['POST'])
@require_admin
def approve_user():
    """å¯©æ ¸ç”¨æˆ¶"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({'success': False, 'message': 'è«‹æä¾›å¯©æ ¸è³‡æ–™'}), 400
        
        # æ”¯æ´å…©ç¨®åƒæ•¸æ ¼å¼ï¼šuserId å’Œ user_id
        user_id = data.get('userId') or data.get('user_id')
        action = data.get('action')  # 'approve' or 'reject'
        
        # ç¢ºä¿user_idæ˜¯æ•´æ•¸é¡å‹
        try:
            if user_id:
                user_id = int(user_id)
        except (ValueError, TypeError):
            return jsonify({
                'success': False, 
                'message': f'ç”¨æˆ¶IDå¿…é ˆæ˜¯æ•¸å­—ï¼Œæ”¶åˆ°: {user_id}'
            }), 400
        
        if not user_id or action not in ['approve', 'reject']:
            return jsonify({
                'success': False, 
                'message': f'ç„¡æ•ˆçš„å¯©æ ¸åƒæ•¸ - user_id: {user_id}, action: {action}'
            }), 400
        
        user = User.query.get(user_id)
        if not user:
            # å¢åŠ è¨ºæ–·ä¿¡æ¯ï¼šæŸ¥çœ‹æ‰€æœ‰ç”¨æˆ¶
            all_users = User.query.all()
            print(f'ğŸ” æ‰¾ä¸åˆ°ç”¨æˆ¶ID {user_id}ï¼Œç¾æœ‰ç”¨æˆ¶:')
            for u in all_users:
                print(f'   ID: {u.id}, ç”¨æˆ¶å: {u.username}, ç‹€æ…‹: {u.status}')
            return jsonify({'success': False, 'message': f'ç”¨æˆ¶ä¸å­˜åœ¨ (ID: {user_id})'}), 404
        
        if user.status != 'pending':
            return jsonify({'success': False, 'message': 'ç”¨æˆ¶ç‹€æ…‹ä¸æ˜¯å¾…å¯©æ ¸'}), 400
        
        # æ›´æ–°ç”¨æˆ¶ç‹€æ…‹
        if action == 'approve':
            user.status = 'approved'
            message = f'ç”¨æˆ¶ {user.name} å·²å¯©æ ¸é€šé'
        else:
            user.status = 'rejected'
            message = f'ç”¨æˆ¶ {user.name} å·²è¢«æ‹’çµ•'
        
        user.updated_at = datetime.utcnow()
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': message
        })
        
    except Exception as e:
        db.session.rollback()
        print(f'å¯©æ ¸ç”¨æˆ¶éŒ¯èª¤: {e}')
        return jsonify({'success': False, 'message': 'å¯©æ ¸å¤±æ•—'}), 500

@main.route('/api/admin/users')
@require_admin
def get_all_users():
    """ç²å–æ‰€æœ‰ç”¨æˆ¶åˆ—è¡¨"""
    try:
        users = User.query.order_by(User.created_at.desc()).all()
        
        result = []
        for user in users:
            result.append({
                'id': user.id,
                'username': user.username,
                'name': user.name,
                'role': user.role,
                'status': user.status,
                'failed_attempts': user.failed_attempts,
                'is_locked': user.is_locked(),
                'created_at': user.created_at.isoformat(),
                'updated_at': user.updated_at.isoformat() if user.updated_at else None
            })
        
        return jsonify({
            'success': True,
            'data': result
        })
        
    except Exception as e:
        print(f'ç²å–ç”¨æˆ¶åˆ—è¡¨éŒ¯èª¤: {e}')
        return jsonify({'success': False, 'message': 'ç²å–ç”¨æˆ¶åˆ—è¡¨å¤±æ•—'}), 500

@main.route('/api/admin/user/<int:user_id>/status', methods=['PUT'])
@require_admin
def update_user_status(user_id):
    """æ›´æ–°ç”¨æˆ¶ç‹€æ…‹"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({'success': False, 'message': 'è«‹æä¾›ç‹€æ…‹è³‡æ–™'}), 400
        
        new_status = data.get('status')
        
        if new_status not in ['pending', 'approved', 'rejected', 'disabled']:
            return jsonify({'success': False, 'message': 'ç„¡æ•ˆçš„ç‹€æ…‹å€¼'}), 400
        
        user = User.query.get(user_id)
        if not user:
            return jsonify({'success': False, 'message': 'ç”¨æˆ¶ä¸å­˜åœ¨'}), 404
        
        # é˜²æ­¢ç®¡ç†å“¡ä¿®æ”¹è‡ªå·±çš„ç‹€æ…‹
        current_user = get_current_user()
        if user.id == current_user.id:
            return jsonify({'success': False, 'message': 'ä¸èƒ½ä¿®æ”¹è‡ªå·±çš„ç‹€æ…‹'}), 400
        
        user.status = new_status
        user.updated_at = datetime.utcnow()
        
        # å¦‚æœå•Ÿç”¨ç”¨æˆ¶ï¼Œé‡ç½®å¤±æ•—æ¬¡æ•¸
        if new_status == 'approved':
            user.reset_failed_attempts()
        
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': f'ç”¨æˆ¶ç‹€æ…‹å·²æ›´æ–°ç‚º {new_status}'
        })
        
    except Exception as e:
        db.session.rollback()
        print(f'æ›´æ–°ç”¨æˆ¶ç‹€æ…‹éŒ¯èª¤: {e}')
        return jsonify({'success': False, 'message': 'æ›´æ–°å¤±æ•—'}), 500

@main.route('/api/admin/user/<int:user_id>/unlock', methods=['POST'])
@require_admin
def unlock_user(user_id):
    """è§£é–ç”¨æˆ¶å¸³è™Ÿ"""
    try:
        user = User.query.get(user_id)
        if not user:
            return jsonify({'success': False, 'message': 'ç”¨æˆ¶ä¸å­˜åœ¨'}), 404
        
        user.reset_failed_attempts()
        user.updated_at = datetime.utcnow()
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': f'ç”¨æˆ¶ {user.name} å·²è§£é–'
        })
        
    except Exception as e:
        db.session.rollback()
        print(f'è§£é–ç”¨æˆ¶éŒ¯èª¤: {e}')
        return jsonify({'success': False, 'message': 'è§£é–å¤±æ•—'}), 500

@main.route('/api/admin/user/<int:user_id>', methods=['DELETE'])
@require_admin
def delete_user(user_id):
    """åˆªé™¤ç”¨æˆ¶"""
    try:
        user = User.query.get(user_id)
        if not user:
            return jsonify({'success': False, 'message': 'ç”¨æˆ¶ä¸å­˜åœ¨'}), 404
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç³»çµ±ç®¡ç†å“¡
        if user.role == 'admin':
            return jsonify({'success': False, 'message': 'ä¸èƒ½åˆªé™¤ç®¡ç†å“¡å¸³è™Ÿ'}), 403
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç•¶å‰ç™»å…¥ç”¨æˆ¶
        current_user_id = session.get('user_id')
        if user.id == current_user_id:
            return jsonify({'success': False, 'message': 'ä¸èƒ½åˆªé™¤ç•¶å‰ç™»å…¥çš„å¸³è™Ÿ'}), 403
        
        # å‚™ä»½ç”¨æˆ¶è³‡æ–™ä»¥ä¾¿æ—¥èªŒè¨˜éŒ„
        deleted_user_name = user.name
        deleted_user_username = user.username
        
        # åˆªé™¤ç”¨æˆ¶ç›¸é—œçš„ç­è¡¨è¨˜éŒ„
        from app.models import Schedule
        Schedule.query.filter_by(employee_id=user.id).delete()
        
        # åˆªé™¤ç”¨æˆ¶
        db.session.delete(user)
        db.session.commit()
        
        return jsonify({
            'success': True,
            'message': f'ç”¨æˆ¶ {deleted_user_name} ({deleted_user_username}) å·²åˆªé™¤'
        })
        
    except Exception as e:
        db.session.rollback()
        print(f'åˆªé™¤ç”¨æˆ¶éŒ¯èª¤: {e}')
        return jsonify({'success': False, 'message': 'åˆªé™¤å¤±æ•—'}), 500

@main.route('/admin')
@require_admin
def admin_page():
    """ç®¡ç†å“¡æ§åˆ¶å°é é¢"""
    return render_template('admin.html')

# ===== æ–°ç‰ˆExcelåŒ¯å…¥é©—è­‰ç³»çµ± v1.0 =====

@main.route('/upload_new', methods=['GET', 'POST'])
@require_admin
def upload_new():
    """æ–°ç‰ˆExcelåŒ¯å…¥é©—è­‰ç³»çµ±"""
    if request.method == 'POST':
        return handle_excel_upload()
    
    # GETè«‹æ±‚ï¼šé¡¯ç¤ºåŒ¯å…¥é é¢
    recent_imports = ImportLog.query.order_by(ImportLog.import_time.desc()).limit(5).all()
    
    # æ·»åŠ ç•¶å‰ç­è¡¨çµ±è¨ˆè³‡æ–™
    from datetime import date, datetime, timedelta
    today = date.today()
    
    # çµ±è¨ˆç•¶å‰è³‡æ–™åº«ä¸­çš„ç­è¡¨è³‡æ–™
    total_schedules = Schedule.query.count()
    total_employees = Employee.query.count()
    
    # æŸ¥æ‰¾æœ€è¿‘çš„æ’ç­è³‡æ–™ç¯„åœ
    latest_schedule = Schedule.query.order_by(Schedule.date.desc()).first()
    earliest_schedule = Schedule.query.order_by(Schedule.date.asc()).first()
    
    # ä»Šæ—¥æ’ç­çµ±è¨ˆ
    today_schedules_count = Schedule.query.filter(Schedule.date == today).count()
    
    # æœ¬æœˆæ’ç­çµ±è¨ˆ
    month_start = date(today.year, today.month, 1)
    if today.month == 12:
        month_end = date(today.year + 1, 1, 1) - timedelta(days=1)
    else:
        month_end = date(today.year, today.month + 1, 1) - timedelta(days=1)
    
    month_schedules_count = Schedule.query.filter(
        Schedule.date >= month_start,
        Schedule.date <= month_end
    ).count()
    
    current_stats = {
        'total_schedules': total_schedules,
        'total_employees': total_employees,
        'today_schedules': today_schedules_count,
        'month_schedules': month_schedules_count,
        'latest_date': latest_schedule.date if latest_schedule else None,
        'earliest_date': earliest_schedule.date if earliest_schedule else None,
        'today': today
    }
    
    return render_template('upload_new.html', 
                         recent_imports=recent_imports, 
                         current_stats=current_stats)

def handle_excel_upload():
    """è™•ç†Excelæª”æ¡ˆä¸Šå‚³å’Œé©—è­‰"""
    try:
        # 1. æª”æ¡ˆé©—è­‰
        if 'file' not in request.files:
            flash('è«‹é¸æ“‡æª”æ¡ˆ', 'error')
            return redirect(url_for('main.upload_new'))
        
        file = request.files['file']
        target_group = request.form.get('target_group')
        skip_invalid = request.form.get('skip_invalid') == 'on'
        
        if file.filename == '':
            flash('è«‹é¸æ“‡æª”æ¡ˆ', 'error')
            return redirect(url_for('main.upload_new'))
        
        if not target_group:
            flash('è«‹é¸æ“‡äººå“¡ç¾¤çµ„', 'error')
            return redirect(url_for('main.upload_new'))
        
        # è‡ªå‹•å¾æª”æ¡ˆåç¨±åˆ¤æ–·è³‡æ–™ç‰ˆæœ¬
        data_version = auto_detect_version(file.filename)
        
        if not file.filename.endswith('.csv'):
            flash('åªæ”¯æ´ CSV æ ¼å¼æª”æ¡ˆ', 'error')
            return redirect(url_for('main.upload_new'))
        
        # 2. è®€å–ç™½åå–®é…ç½®
        whitelist_data = load_whitelist()
        if not whitelist_data:
            flash('ç„¡æ³•è¼‰å…¥ç™½åå–®é…ç½®', 'error')
            return redirect(url_for('main.upload_new'))
        
        # 3. è®€å–å’Œè§£æExcel
        df = read_excel_file(file)
        if df is None:
            flash('ç„¡æ³•è®€å–Excelæª”æ¡ˆï¼Œè«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼', 'error')
            return redirect(url_for('main.upload_new'))
        
        # 4. è³‡æ–™é è¦½ï¼ˆå‰50ç­†ï¼‰
        preview_data = create_preview_data_v11(df, target_group, whitelist_data)
        
        # 5. è³‡æ–™é©—è­‰
        validation_result = validate_excel_data_v11(df, data_version, file.filename, target_group, whitelist_data)
        
        # 6. ç‚ºæ–°çš„åŒ¯å…¥æ–¹å¼æº–å‚™base64ç·¨ç¢¼çš„CSVè³‡æ–™
        import base64
        csv_string = df.to_csv(index=False)
        csv_base64 = base64.b64encode(csv_string.encode('utf-8')).decode('utf-8')
        validation_result['csv_data'] = csv_base64
        validation_result['target_group'] = target_group
        validation_result['filename'] = file.filename
        
        # 7. é¡¯ç¤ºçµæœé é¢
        recent_imports = ImportLog.query.order_by(ImportLog.import_time.desc()).limit(5).all()
        
        return render_template('upload_new.html', 
                             preview_data=preview_data,
                             validation_result=validation_result,
                             recent_imports=recent_imports)
        
    except Exception as e:
        flash(f'è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}', 'error')
        return redirect(url_for('main.upload_new'))

def read_excel_file(file):
    """è®€å–CSVæª”æ¡ˆä¸¦è¿”å›DataFrame"""
    return read_csv_file(file)

def read_csv_file(file):
    """è®€å–CSVæª”æ¡ˆä¸¦è¿”å›DataFrame"""
    try:
        # å˜—è©¦ä¸åŒçš„ç·¨ç¢¼
        encodings = ['utf-8', 'utf-8-sig', 'big5', 'gbk', 'cp950']
        
        for encoding in encodings:
            try:
                # é‡ç½®æª”æ¡ˆæŒ‡é‡
                file.seek(0)
                
                # å˜—è©¦è®€å–CSV
                df = pd.read_csv(file, encoding=encoding)
                
                # æª¢æŸ¥æ˜¯å¦åŒ…å«å¿…è¦æ¬„ä½
                columns_str = '|'.join(str(col).lower() for col in df.columns)
                if any(keyword in columns_str for keyword in ['å§“å', 'å“¡å·¥', 'æ—¥æœŸ', 'ç­']):
                    print(f'âœ… CSVæª”æ¡ˆä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–æˆåŠŸ')
                    return df
                    
                # å¦‚æœç¬¬ä¸€è¡Œä¸æ˜¯æ¨™é¡Œï¼Œå˜—è©¦è·³éå‰å¹¾è¡Œ
                for skip_rows in [1, 2, 3, 4]:
                    try:
                        file.seek(0)
                        df_skip = pd.read_csv(file, encoding=encoding, skiprows=skip_rows)
                        columns_str = '|'.join(str(col).lower() for col in df_skip.columns)
                        if any(keyword in columns_str for keyword in ['å§“å', 'å“¡å·¥', 'æ—¥æœŸ', 'ç­']):
                            print(f'âœ… CSVæª”æ¡ˆä½¿ç”¨ {encoding} ç·¨ç¢¼ï¼Œè·³é {skip_rows} è¡Œè®€å–æˆåŠŸ')
                            return df_skip
                    except:
                        continue
                
            except UnicodeDecodeError:
                continue
            except Exception as e:
                print(f'ä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–CSVå¤±æ•—: {e}')
                continue
        
        # å¦‚æœæ‰€æœ‰ç·¨ç¢¼éƒ½å¤±æ•—ï¼Œä½¿ç”¨é è¨­UTF-8
        file.seek(0)
        df = pd.read_csv(file, encoding='utf-8')
        print('âš ï¸ ä½¿ç”¨é è¨­UTF-8ç·¨ç¢¼è®€å–CSV')
        return df
        
    except Exception as e:
        print(f'è®€å–CSVæª”æ¡ˆéŒ¯èª¤: {e}')
        return None


def create_preview_data(df):
    """å‰µå»ºè³‡æ–™é è¦½ï¼ˆå‰10ç­†ï¼‰"""
    preview_data = []
    
    # å˜—è©¦è­˜åˆ¥æ¬„ä½
    name_col, date_col, shift_col = identify_columns(df)
    
    if not all([name_col, date_col, shift_col]):
        # å¦‚æœç„¡æ³•è­˜åˆ¥æ¬„ä½ï¼Œé¡¯ç¤ºåŸå§‹è³‡æ–™
        for i in range(min(10, len(df))):
            row = df.iloc[i]
            preview_data.append({
                'å§“å': str(row.iloc[0]) if len(row) > 0 else '',
                'æ—¥æœŸ': str(row.iloc[1]) if len(row) > 1 else '',
                'ç­åˆ¥': str(row.iloc[2]) if len(row) > 2 else ''
            })
    else:
        # ä½¿ç”¨è­˜åˆ¥çš„æ¬„ä½
        for i in range(min(10, len(df))):
            row = df.iloc[i]
            preview_data.append({
                'å§“å': str(row[name_col]) if pd.notna(row[name_col]) else '',
                'æ—¥æœŸ': str(row[date_col]) if pd.notna(row[date_col]) else '',
                'ç­åˆ¥': str(row[shift_col]) if pd.notna(row[shift_col]) else ''
            })
    
    return preview_data

def identify_columns(df):
    """æ™ºèƒ½è­˜åˆ¥CSVæ ¼å¼ä¸¦æå–æ¬„ä½ - æ”¯æ´3æ¬„ä½å’Œ5æ¬„ä½æ ¼å¼"""
    # åˆå§‹åŒ–æ¬„ä½è®Šæ•¸
    name_col = None
    date_col = None
    shift_col = None
    employee_code_col = None
    year_month_col = None
    day_col = None
    
    # èª¿è©¦è¼¸å‡º
    print(f"ğŸ” CSVæ¬„ä½: {list(df.columns)}")
    print(f"ğŸ” æ¬„ä½æ•¸é‡: {len(df.columns)}")
    print(f"ğŸ” ç¬¬ä¸€è¡Œè³‡æ–™: {list(df.iloc[0]) if len(df) > 0 else 'N/A'}")
    
    # åˆ¤æ–·CSVæ ¼å¼é¡å‹
    csv_format = detect_csv_format(df)
    print(f"ğŸ¯ åµæ¸¬åˆ°CSVæ ¼å¼: {csv_format}")
    
    if csv_format == "5_column":
        # 5æ¬„ä½æ ¼å¼: å§“å, å“¡å·¥ä»£ç¢¼, å¹´æœˆ, æ—¥æœŸ, ç­åˆ¥
        return identify_5_column_format(df)
    elif csv_format == "3_column":
        # 3æ¬„ä½æ ¼å¼: å§“å, æ—¥æœŸ, ç­åˆ¥ (åŸæœ‰æ ¼å¼)
        return identify_3_column_format(df)
    else:
        # å˜—è©¦é€šç”¨è­˜åˆ¥
        return identify_columns_generic(df)

def detect_csv_format(df):
    """åµæ¸¬CSVæ ¼å¼é¡å‹"""
    columns = list(df.columns)
    col_count = len(columns)
    
    # æª¢æŸ¥5æ¬„ä½æ ¼å¼ç‰¹å¾µ
    if col_count >= 5:
        col_names_lower = [str(col).lower().strip() for col in columns]
        
        # å°‹æ‰¾5æ¬„ä½æ ¼å¼çš„é—œéµæ¬„ä½
        has_employee_code = any('å“¡å·¥ä»£ç¢¼' in col or 'ä»£ç¢¼' in col or 'code' in col.lower() for col in col_names_lower)
        has_year_month = any('å¹´æœˆ' in col for col in col_names_lower)
        has_day = any(col in ['æ—¥æœŸ', 'day', 'æ—¥'] for col in col_names_lower)
        
        if has_employee_code or has_year_month:
            return "5_column"
    
    # æª¢æŸ¥3æ¬„ä½æ ¼å¼ç‰¹å¾µ
    if col_count == 3:
        col_names_lower = [str(col).lower().strip() for col in columns]
        has_name = any(keyword in col for col in col_names_lower for keyword in ['å§“å', 'name', 'å“¡å·¥'])
        has_date = any(keyword in col for col in col_names_lower for keyword in ['æ—¥æœŸ', 'date'])
        has_shift = any(keyword in col for col in col_names_lower for keyword in ['ç­åˆ¥', 'shift', 'ç­æ¬¡'])
        
        if has_name and has_date and has_shift:
            return "3_column"
    
    return "unknown"

def identify_5_column_format(df):
    """è­˜åˆ¥5æ¬„ä½æ ¼å¼: å§“å, å“¡å·¥ä»£ç¢¼, å¹´æœˆ, æ—¥æœŸ, ç­åˆ¥"""
    name_col = None
    employee_code_col = None
    year_month_col = None
    day_col = None
    shift_col = None
    
    for col in df.columns:
        col_str = str(col).lower().strip()
        
        # å§“åæ¬„ä½
        if col_str in ['å§“å', 'å“¡å·¥å§“å', 'name', 'åå­—'] and not name_col:
            name_col = col
            print(f"âœ… æ‰¾åˆ°å§“åæ¬„ä½: {col}")
        
        # å“¡å·¥ä»£ç¢¼æ¬„ä½
        elif ('å“¡å·¥ä»£ç¢¼' in col_str or 'ä»£ç¢¼' in col_str or 'code' in col_str) and not employee_code_col:
            employee_code_col = col
            print(f"âœ… æ‰¾åˆ°å“¡å·¥ä»£ç¢¼æ¬„ä½: {col}")
        
        # å¹´æœˆæ¬„ä½
        elif 'å¹´æœˆ' in col_str and not year_month_col:
            year_month_col = col
            print(f"âœ… æ‰¾åˆ°å¹´æœˆæ¬„ä½: {col}")
        
        # æ—¥æœŸæ¬„ä½ (å–®ç´”çš„æ—¥)
        elif col_str in ['æ—¥æœŸ', 'day', 'æ—¥'] and not day_col:
            day_col = col
            print(f"âœ… æ‰¾åˆ°æ—¥æœŸæ¬„ä½: {col}")
        
        # ç­åˆ¥æ¬„ä½
        elif col_str in ['ç­åˆ¥', 'ç­æ¬¡', 'shift', 'æ’ç­'] and not shift_col:
            shift_col = col
            print(f"âœ… æ‰¾åˆ°ç­åˆ¥æ¬„ä½: {col}")
    
    print(f"ğŸ¯ 5æ¬„ä½æ ¼å¼è­˜åˆ¥çµæœ: å§“å={name_col}, å“¡å·¥ä»£ç¢¼={employee_code_col}, å¹´æœˆ={year_month_col}, æ—¥æœŸ={day_col}, ç­åˆ¥={shift_col}")
    
    # è¿”å›åŒ…å«æ ¼å¼è³‡è¨Šçš„çµæœ
    return {
        'format': '5_column',
        'name_col': name_col,
        'employee_code_col': employee_code_col,
        'year_month_col': year_month_col,
        'day_col': day_col,
        'shift_col': shift_col,
        'date_col': None  # 5æ¬„ä½æ ¼å¼ä¸­éœ€è¦çµ„åˆå¹´æœˆå’Œæ—¥æœŸ
    }

def identify_3_column_format(df):
    """è­˜åˆ¥3æ¬„ä½æ ¼å¼: å§“å, æ—¥æœŸ, ç­åˆ¥ï¼ˆåŸæœ‰æ ¼å¼ï¼‰"""
    name_col = None
    date_col = None
    shift_col = None
    
    # å„ªå…ˆè™•ç†ç›´å¼æ ¼å¼ - æª¢æŸ¥column headers
    for col in df.columns:
        col_str = str(col).lower().strip()
        
        # å§“åæ¬„ä½è­˜åˆ¥
        if col_str in ['å§“å', 'å“¡å·¥å§“å', 'name', 'åå­—'] and not name_col:
            name_col = col
            print(f"âœ… æ‰¾åˆ°å§“åæ¬„ä½: {col}")
        
        # æ—¥æœŸæ¬„ä½è­˜åˆ¥
        elif col_str in ['æ—¥æœŸ', 'date', 'æ™‚é–“', 'datetime'] and not date_col:
            date_col = col
            print(f"âœ… æ‰¾åˆ°æ—¥æœŸæ¬„ä½: {col}")
        
        # ç­åˆ¥æ¬„ä½è­˜åˆ¥
        elif col_str in ['ç­åˆ¥', 'ç­æ¬¡', 'shift', 'æ’ç­'] and not shift_col:
            shift_col = col
            print(f"âœ… æ‰¾åˆ°ç­åˆ¥æ¬„ä½: {col}")
    
    # å¦‚æœç›´å¼æ ¼å¼è­˜åˆ¥å¤±æ•—ï¼Œå˜—è©¦æ©«å¼æ ¼å¼ï¼ˆæ•¸å­—æ¬„ä½ï¼‰
    if not date_col:
        numeric_cols = [col for col in df.columns if str(col).strip().isdigit()]
        if numeric_cols:
            date_col = numeric_cols[0]  # å–ç¬¬ä¸€å€‹æ•¸å­—æ¬„ä½ä½œç‚ºæ—¥æœŸ
            print(f"âš¡ ä½¿ç”¨æ•¸å­—æ¬„ä½ä½œç‚ºæ—¥æœŸï¼ˆæ©«å¼æ ¼å¼ï¼‰: {date_col}")
    
    print(f"ğŸ¯ 3æ¬„ä½æ ¼å¼è­˜åˆ¥çµæœ: å§“å={name_col}, æ—¥æœŸ={date_col}, ç­åˆ¥={shift_col}")
    
    # è¿”å›èˆ‡åŸæœ‰æ ¼å¼å…¼å®¹çš„çµæœ
    return {
        'format': '3_column',
        'name_col': name_col,
        'date_col': date_col,  
        'shift_col': shift_col,
        'employee_code_col': None,
        'year_month_col': None,
        'day_col': None
    }

def identify_columns_generic(df):
    """é€šç”¨æ¬„ä½è­˜åˆ¥ï¼ˆå‘å¾Œå…¼å®¹ï¼‰"""
    name_col = None
    date_col = None  
    shift_col = None
    
    for col in df.columns:
        col_str = str(col).lower().strip()
        
        if col_str in ['å§“å', 'å“¡å·¥å§“å', 'name', 'åå­—'] and not name_col:
            name_col = col
        elif col_str in ['æ—¥æœŸ', 'date', 'æ™‚é–“', 'datetime'] and not date_col:
            date_col = col
        elif col_str in ['ç­åˆ¥', 'ç­æ¬¡', 'shift', 'æ’ç­'] and not shift_col:
            shift_col = col
    
    # å¦‚æœä»æœªæ‰¾åˆ°å¿…è¦æ¬„ä½ï¼Œæª¢æŸ¥ç¬¬ä¸€è¡Œè³‡æ–™æ˜¯å¦ç‚ºæ¬„ä½åç¨±
    if not all([name_col, date_col, shift_col]) and len(df) > 0:
        first_row = df.iloc[0]
        for idx, cell in enumerate(first_row):
            cell_str = str(cell).strip()
            if cell_str == 'å§“å' and not name_col:
                name_col = df.columns[idx]
    
    return {
        'format': '3_column',  # é»˜èªç‚º3æ¬„ä½å…¼å®¹æ ¼å¼
        'name_col': name_col,
        'date_col': date_col,
        'shift_col': shift_col,
        'employee_code_col': None,
        'year_month_col': None,
        'day_col': None
    }

def combine_date_from_5_column(year_month_value, day_value):
    """å¾5æ¬„ä½æ ¼å¼çš„å¹´æœˆå’Œæ—¥æœŸçµ„åˆå®Œæ•´æ—¥æœŸ"""
    try:
        if pd.isna(year_month_value) or pd.isna(day_value):
            return None
            
        year_month_str = str(year_month_value).strip()
        day_str = str(day_value).strip()
        
        # è™•ç†å¹´æœˆæ ¼å¼ (ä¾‹å¦‚: "2024-10", "2024/10", "202410", "114/08"æ°‘åœ‹å¹´)
        year = None
        month = None
        
        if '-' in year_month_str:
            year, month = year_month_str.split('-', 1)
        elif '/' in year_month_str:
            year, month = year_month_str.split('/', 1)
        elif len(year_month_str) == 6 and year_month_str.isdigit():  # 202410
            year = year_month_str[:4]
            month = year_month_str[4:]
        else:
            print(f"âš ï¸ ç„¡æ³•è§£æå¹´æœˆæ ¼å¼: {year_month_str}")
            return None
        
        # è™•ç†å¹´ä»½æ ¼å¼
        year = year.strip()
        month = month.strip()
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ°‘åœ‹å¹´æ ¼å¼ (ä¾‹å¦‚: 114 = æ°‘åœ‹114å¹´ = è¥¿å…ƒ2025å¹´)
        if len(year) <= 3 and year.isdigit():
            year_int = int(year)
            if year_int > 10 and year_int < 200:  # å‡è¨­æ˜¯æ°‘åœ‹å¹´
                year = str(year_int + 1911)  # è½‰æ›ç‚ºè¥¿å…ƒå¹´
                print(f"ğŸ—“ï¸ åµæ¸¬åˆ°æ°‘åœ‹å¹´ï¼Œè½‰æ›: æ°‘åœ‹{year_int}å¹´ â†’ è¥¿å…ƒ{year}å¹´")
        
        # è™•ç†æ—¥æœŸ (å»é™¤å‰å°é›¶)
        day = day_str.lstrip('0') or '1'  # å¦‚æœå…¨æ˜¯0ï¼Œè¨­ç‚º1
        
        # é©—è­‰å¹´ä»½æ ¼å¼
        if not year.isdigit() or len(year) != 4:
            print(f"âš ï¸ å¹´ä»½æ ¼å¼éŒ¯èª¤: {year}")
            return None
        
        # é©—è­‰æœˆä»½æ ¼å¼
        if not month.isdigit() or int(month) < 1 or int(month) > 12:
            print(f"âš ï¸ æœˆä»½æ ¼å¼éŒ¯èª¤: {month}")
            return None
        
        # çµ„åˆå®Œæ•´æ—¥æœŸ
        full_date_str = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # é©—è­‰æ—¥æœŸæœ‰æ•ˆæ€§
        combined_date = datetime.strptime(full_date_str, '%Y-%m-%d').date()
        print(f"ğŸ“… æ—¥æœŸçµ„åˆæˆåŠŸ: {year_month_str} + {day_str} = {combined_date}")
        
        return combined_date
        
    except Exception as e:
        print(f"âŒ æ—¥æœŸçµ„åˆå¤±æ•—: {year_month_str} + {day_str}, éŒ¯èª¤: {e}")
        return None

def get_date_value_enhanced(row, columns_info):
    """æ ¹æ“šæ ¼å¼ç²å–æ—¥æœŸå€¼ - æ”¯æ´3æ¬„ä½å’Œ5æ¬„ä½æ ¼å¼"""
    csv_format = columns_info.get('format', '3_column')
    
    if csv_format == '5_column':
        # 5æ¬„ä½æ ¼å¼: éœ€è¦çµ„åˆå¹´æœˆå’Œæ—¥æœŸ
        year_month_col = columns_info.get('year_month_col')
        day_col = columns_info.get('day_col')
        
        if year_month_col and day_col:
            year_month_value = row[year_month_col] if pd.notna(row[year_month_col]) else None
            day_value = row[day_col] if pd.notna(row[day_col]) else None
            return combine_date_from_5_column(year_month_value, day_value)
        else:
            return None
    else:
        # 3æ¬„ä½æ ¼å¼: ç›´æ¥ä½¿ç”¨æ—¥æœŸæ¬„ä½
        date_col = columns_info.get('date_col')
        if date_col:
            return row[date_col] if pd.notna(row[date_col]) else None
        else:
            return None

def validate_excel_data(df, data_version, filename):
    """é©—è­‰Excelè³‡æ–™"""
    result = {
        'status': 'OK',
        'total_records': len(df),
        'valid_records': 0,
        'warnings': 0,
        'errors': 0,
        'error_messages': [],
        'filename': filename,
        'data_version': data_version
    }
    
    try:
        # 1. æ¬„ä½é©—è­‰
        name_col, date_col, shift_col = identify_columns(df)
        
        if not name_col:
            result['errors'] += 1
            result['error_messages'].append('æ‰¾ä¸åˆ°å§“åæ¬„ä½ï¼ˆæ‡‰åŒ…å«ï¼šå§“åã€å“¡å·¥å§“åæˆ–nameï¼‰')
            result['status'] = 'ERROR'
        
        if not date_col:
            result['errors'] += 1
            result['error_messages'].append('æ‰¾ä¸åˆ°æ—¥æœŸæ¬„ä½ï¼ˆæ‡‰åŒ…å«ï¼šæ—¥æœŸæˆ–dateï¼‰')
            result['status'] = 'ERROR'
        
        if not shift_col:
            result['errors'] += 1
            result['error_messages'].append('æ‰¾ä¸åˆ°ç­åˆ¥æ¬„ä½ï¼ˆæ‡‰åŒ…å«ï¼šç­åˆ¥ã€ç­æ¬¡æˆ–shiftï¼‰')
            result['status'] = 'ERROR'
        
        # å¦‚æœåŸºæœ¬æ¬„ä½éƒ½æ‰¾ä¸åˆ°ï¼Œç›´æ¥è¿”å›
        if result['status'] == 'ERROR':
            return result
        
        # 2. è³‡æ–™å…§å®¹é©—è­‰
        valid_shifts = get_valid_shift_codes()
        target_employees = get_target_employees()
        
        for index, row in df.iterrows():
            try:
                # å§“åé©—è­‰
                employee_name = str(row[name_col]).strip() if pd.notna(row[name_col]) else ''
                if not employee_name:
                    result['errors'] += 1
                    result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šå§“åæ¬„ä½ç©ºç™½')
                    continue
                
                # ç¾¤çµ„éæ¿¾ï¼šåªè™•ç†å±¬æ–¼ç›®æ¨™ç¾¤çµ„çš„å“¡å·¥ï¼Œè·³éå…¶ä»–å“¡å·¥
                if employee_name not in target_employees:
                    # è·³éä¸åœ¨ç›®æ¨™ç¾¤çµ„ä¸­çš„å“¡å·¥ï¼Œä¸é¡¯ç¤ºéŒ¯èª¤è¨Šæ¯
                    continue
                
                # æ—¥æœŸé©—è­‰
                date_value = row[date_col] if pd.notna(row[date_col]) else None
                if not date_value:
                    result['errors'] += 1
                    result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šæ—¥æœŸæ¬„ä½ç©ºç™½')
                    continue
                
                # ç­åˆ¥é©—è­‰
                shift_code = str(row[shift_col]).strip() if pd.notna(row[shift_col]) else ''
                if not shift_code:
                    result['errors'] += 1
                    result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šç­åˆ¥æ¬„ä½ç©ºç™½')
                    continue
                
                if shift_code not in valid_shifts:
                    result['warnings'] += 1
                    result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šæœªçŸ¥ç­åˆ¥ä»£ç¢¼ {shift_code}')
                    if result['status'] == 'OK':
                        result['status'] = 'WARNING'
                
                result['valid_records'] += 1
                
            except Exception as e:
                result['errors'] += 1
                result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šè³‡æ–™è™•ç†éŒ¯èª¤ - {str(e)}')
        
        # 3. è¨­å®šæœ€çµ‚ç‹€æ…‹
        if result['errors'] > 0:
            result['status'] = 'ERROR'
        elif result['warnings'] > 0:
            result['status'] = 'WARNING'
        
        return result
        
    except Exception as e:
        result['status'] = 'ERROR'
        result['errors'] += 1
        result['error_messages'].append(f'é©—è­‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}')
        return result

def get_valid_shift_codes():
    """ç²å–æœ‰æ•ˆç­åˆ¥ä»£ç¢¼ - å¾è³‡æ–™åº«å‹•æ…‹è¼‰å…¥"""
    try:
        from app.models import ShiftType
        shift_codes = [shift.code for shift in ShiftType.query.all()]
        print(f"âœ… å¾è³‡æ–™åº«è¼‰å…¥ {len(shift_codes)} å€‹ç­åˆ¥ä»£ç¢¼: {shift_codes}")
        return shift_codes
    except Exception as e:
        print(f"âš ï¸ è¼‰å…¥ç­åˆ¥ä»£ç¢¼å¤±æ•—ï¼Œä½¿ç”¨é è¨­æ¸…å–®: {e}")
        # é è¨­æ¸…å–®ä½œç‚ºå‚™ç”¨ï¼ˆåŒ…å«æ‰€æœ‰æˆ‘å€‘å·²åˆå§‹åŒ–çš„ç­åˆ¥ï¼‰
        return [
            'A', 'B', 'C', 'OFF', 'FC', 'FC/å·¥ç¨‹', 'FC/æ€¥æ•‘èª²', 'FX',
            'P1s', 'P1s2', 'P1c', 'P1c2', 'P1n', 'P1n/å¤œè¶…', 'P1p', 'P1p2', 'P1p/ME',
            'P2s', 'P2c', 'P2n', 'P2p', 'P2p/LD',
            'P3c', 'P3n', 'P3n/å¤œè¶…', 'P3p',
            'P4c', 'P4n', 'P4p',
            'P5', 'P6', 'C1', 'C3', 'N1', 'N2', 'E1', 'R1',
            'H0', 'H1', 'èˆå°'
        ]

def load_whitelist():
    """è¼‰å…¥ç™½åå–®é…ç½® - æ•´åˆç¾¤çµ„ç®¡ç†ç³»çµ±"""
    try:
        # å…ˆå˜—è©¦å¾ç¾¤çµ„ç®¡ç†ç³»çµ±è¼‰å…¥
        groups = GroupMembers.query.all()
        if groups:
            group_data = {}
            allowed_names = set()
            
            for group in groups:
                members = group.get_members()
                group_data[group.group_name] = members
                allowed_names.update(members)
            
            # æ·»åŠ èˆŠç‰ˆæœ¬å…¼å®¹çš„ç¾¤çµ„åç¨±
            legacy_mapping = {
                'æ¼”å‡ºäººå“¡': group_data.get('çµ±ç±Œçµ„', []) + group_data.get('èˆå°çµ„', []),
                'æŠ€è¡“äººå“¡': group_data.get('ç‡ˆå…‰çµ„', []) + group_data.get('è¦–è½çµ„', []) + group_data.get('ç¶­è­·çµ„', []),
                'ç‡ˆå…‰çµ„': group_data.get('ç‡ˆå…‰çµ„', []),
                'å…¨åå–®': list(allowed_names)
            }
            
            result = {
                'group': {**group_data, **legacy_mapping},
                'allowed': list(allowed_names)
            }
            
            print(f"âœ… å¾ç¾¤çµ„ç®¡ç†ç³»çµ±è¼‰å…¥ç™½åå–®ï¼Œå…± {len(allowed_names)} äºº")
            return result
        
        # å¦‚æœç¾¤çµ„ç®¡ç†ç³»çµ±æ²’æœ‰è³‡æ–™ï¼Œå›é€€åˆ°èˆŠçš„JSONæª”æ¡ˆ
        whitelist_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'whitelist.json')
        with open(whitelist_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print("âš ï¸ ä½¿ç”¨èˆŠç‰ˆJSONç™½åå–®æª”æ¡ˆ")
            return data
            
    except Exception as e:
        print(f'è¼‰å…¥ç™½åå–®é…ç½®éŒ¯èª¤: {e}')
        # è¿”å›ç©ºçš„çµæ§‹è€Œä¸æ˜¯None
        return {'group': {}, 'allowed': []}

def auto_detect_version(filename):
    """æ ¹æ“šæª”æ¡ˆåç¨±è‡ªå‹•æª¢æ¸¬è³‡æ–™ç‰ˆæœ¬ - çµ±ä¸€ä½¿ç”¨ä¸€èˆ¬ç‰ˆæœ¬"""
    return 'ä¸€èˆ¬ç‰ˆæœ¬'

def create_preview_data_v11(df, target_group, whitelist_data):
    """å‰µå»ºv1.2ç‰ˆæœ¬çš„è³‡æ–™é è¦½ - æ”¯æ´3æ¬„ä½å’Œ5æ¬„ä½æ ¼å¼"""
    preview_data = []
    
    # ç‡ˆå…‰çµ„æ ¸å¿ƒ7äººåå–®
    core_light_crew = ['è³´ ç§‰ å®', 'æ æƒŸ ç¶±', 'æ å®¶ ç‘‹', 'ç‹ å¿— å¿ ', 'é¡§ è‚² ç¦', 'èƒ¡ ç¿Š æ½”', 'æœ± å®¶ å¾·']
    
    # å˜—è©¦è­˜åˆ¥æ¬„ä½ - ä½¿ç”¨æ–°çš„æ™ºèƒ½è­˜åˆ¥ç³»çµ±
    columns_info = identify_columns(df)
    valid_shifts = get_valid_shift_codes()
    
    print(f"ğŸ” é è¦½è³‡æ–™ä½¿ç”¨æ ¼å¼: {columns_info.get('format', 'unknown')}")
    
    # æª¢æŸ¥å¿…è¦æ¬„ä½æ˜¯å¦å­˜åœ¨
    name_col = columns_info.get('name_col')
    shift_col = columns_info.get('shift_col')
    
    if name_col and shift_col:
        # ç²å–ç›®æ¨™ç¾¤çµ„çš„å“¡å·¥åå–®
        if target_group == 'å…¨åå–®':
            # çœŸæ­£çš„å…¨åå–®ï¼šä¸ä½¿ç”¨ç™½åå–®é™åˆ¶ï¼Œå¾CSVä¸­æå–æ‰€æœ‰å”¯ä¸€å§“å
            valid_names = df[name_col].dropna().astype(str).str.strip().unique().tolist()
            print(f"ğŸ”§ [èª¿è©¦] å…¨åå–®æ¨¡å¼ï¼šå¾CSVæå–åˆ° {len(valid_names)} å€‹å”¯ä¸€å§“å")
        else:
            valid_names = whitelist_data.get('group', {}).get(target_group, [])
        
        record_count = 0
        for i in range(len(df)):
            if record_count >= 50:  # é™åˆ¶é è¦½50ç­†
                break
                
            row = df.iloc[i]
            employee_name = str(row[name_col]).strip() if pd.notna(row[name_col]) else ''
            
            # åªé¡¯ç¤ºç›®æ¨™ç¾¤çµ„çš„è¨˜éŒ„
            if employee_name not in valid_names:
                continue
            
            record_count += 1
            issues = []
            status = 'ok'
            
            # ä½¿ç”¨å¢å¼·çš„æ—¥æœŸç²å–åŠŸèƒ½
            date_value = get_date_value_enhanced(row, columns_info)
            shift_code = str(row[shift_col]).strip() if pd.notna(row[shift_col]) else ''
            
            # ç²å–å“¡å·¥ä»£ç¢¼ï¼ˆå¦‚æœæ˜¯5æ¬„ä½æ ¼å¼çš„è©±ï¼‰
            employee_code = ''
            if columns_info.get('format') == '5_column' and columns_info.get('employee_code_col'):
                employee_code = str(row[columns_info['employee_code_col']]).strip() if pd.notna(row[columns_info['employee_code_col']]) else ''
            
            # æ ¼å¼åŒ–æ—¥æœŸ
            formatted_date = ''
            if date_value:
                try:
                    if isinstance(date_value, str):
                        parsed_date = datetime.strptime(date_value, '%Y-%m-%d').date()
                        formatted_date = parsed_date.strftime('%Y/%m/%d')
                    elif hasattr(date_value, 'strftime'):
                        # å¦‚æœæ˜¯dateå°è±¡æˆ–datetimeå°è±¡
                        if hasattr(date_value, 'date'):
                            formatted_date = date_value.date().strftime('%Y/%m/%d')
                        else:
                            formatted_date = date_value.strftime('%Y/%m/%d')
                    else:
                        formatted_date = str(date_value)
                        if formatted_date and formatted_date != 'nan':
                            issues.append('æ—¥æœŸæ ¼å¼ç„¡æ³•è§£æ')
                            status = 'warning'
                except Exception as e:
                    formatted_date = str(date_value)
                    issues.append(f'æ—¥æœŸæ ¼å¼éŒ¯èª¤: {str(e)[:50]}')
                    status = 'error'
            else:
                issues.append('æ—¥æœŸæ¬„ä½ç©ºç™½')
                status = 'error'
            
            # é©—è­‰å§“å
            if not employee_name:
                issues.append('å§“åæ¬„ä½ç©ºç™½')
                status = 'error'
            
            # é©—è­‰ç­åˆ¥
            if not shift_code:
                issues.append('ç­åˆ¥æ¬„ä½ç©ºç™½')
                status = 'error'
            elif shift_code not in valid_shifts:
                issues.append(f'ç­åˆ¥ä»£è™ŸéŒ¯èª¤ï¼š{shift_code}')
                status = 'error'
            
            preview_data.append({
                'row': i + 2,
                'name': employee_name,
                'employee_code': employee_code,  # æ–°å¢å“¡å·¥ä»£ç¢¼æ¬„ä½
                'date': formatted_date if formatted_date else str(date_value),
                'shift': shift_code,
                'status': status,
                'message': '; '.join(issues) if issues else '',
                'format': columns_info.get('format', '3_column')  # è¨˜éŒ„æ ¼å¼è³‡è¨Š
            })
        
        return preview_data
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºæ©«å¼æ ¼å¼ï¼ˆæ•¸å­—æ¬„ä½ä»£è¡¨æ—¥æœŸï¼‰
    numeric_cols = [col for col in df.columns if str(col).strip().isdigit()]
    
    if numeric_cols:
        # è™•ç†æ©«å¼æ ¼å¼
        for i in range(len(df)):
            row = df.iloc[i]
            
            # å˜—è©¦å¤šç¨®æ–¹å¼æ‰¾åˆ°å§“å
            employee_name = ''
            
            # æ–¹å¼1: ä½¿ç”¨è­˜åˆ¥åˆ°çš„å§“åæ¬„ä½
            if name_col and name_col in df.columns:
                employee_name = str(row[name_col]).strip() if pd.notna(row[name_col]) else ''
            
            # æ–¹å¼2: æª¢æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦æœ‰å§“åè³‡è¨Šï¼Œå¦‚æœæœ‰ï¼Œä½¿ç”¨å°æ‡‰ä½ç½®
            if not employee_name and len(df) > 0:
                first_row = df.iloc[0]
                for col_idx, header_value in enumerate(first_row):
                    if str(header_value).strip() == 'å§“å' and col_idx < len(row):
                        employee_name = str(row.iloc[col_idx]).strip() if pd.notna(row.iloc[col_idx]) else ''
                        break
            
            # æ–¹å¼3: å˜—è©¦ç¬¬ä¸€æ¬„
            if not employee_name:
                employee_name = str(row.iloc[0]).strip() if len(row) > 0 and pd.notna(row.iloc[0]) else ''
            
            # è·³éæ¨™é¡Œè¡Œå’Œéæ ¸å¿ƒ7äººè¨˜éŒ„
            if employee_name in ['å§“å', 'å¹´æœˆ', 'çµ„å®¤åˆ¥', 'å“¡å·¥ä»£ç¢¼'] or employee_name not in core_light_crew:
                continue
            
            # éæ­·æ¯å€‹æ•¸å­—æ¬„ä½ï¼ˆæ—¥æœŸï¼‰ï¼Œåªé è¦½å‰10å¤©
            for day_col in numeric_cols[:10]:
                if day_col in row.index:
                    shift_value = str(row[day_col]).strip() if pd.notna(row[day_col]) else ''
                    
                    if shift_value and shift_value not in ['nan', '']:
                        issues = []
                        status = 'ok'
                        
                        if shift_value not in valid_shifts:
                            issues.append(f'ç­åˆ¥ä»£è™ŸéŒ¯èª¤ï¼š{shift_value}')
                            status = 'error'
                        
                        try:
                            day_num = int(day_col)
                            formatted_date = f'2025-07-{day_num:02d}'
                        except:
                            formatted_date = str(day_col)
                        
                        preview_data.append({
                            'row': i + 2,
                            'name': employee_name,
                            'date': formatted_date,
                            'shift': shift_value,
                            'status': status,
                            'message': '; '.join(issues) if issues else ''
                        })
                        
                        if len(preview_data) >= 50:  # é™åˆ¶é è¦½æ•¸é‡
                            return preview_data
    
    # å¦‚æœç„¡æ³•è­˜åˆ¥ä»»ä½•æ ¼å¼ï¼Œé¡¯ç¤ºåŸºæœ¬è³‡è¨Š
    if not preview_data:
        for i in range(min(10, len(df))):
            row = df.iloc[i]
            preview_data.append({
                'row': i + 2,
                'name': str(row.iloc[0]) if len(row) > 0 else '',
                'date': str(row.iloc[1]) if len(row) > 1 else '',
                'shift': str(row.iloc[2]) if len(row) > 2 else '',
                'status': 'warning',
                'message': 'ç„¡æ³•è­˜åˆ¥æª”æ¡ˆæ ¼å¼'
            })
    
    return preview_data

def normalize_shift_code(shift_code):
    """æ¨™æº–åŒ–ç­åˆ¥ä»£è™Ÿï¼Œè‡ªå‹•è½‰æ›å«æœ‰èªªæ˜çš„ç­åˆ¥"""
    if not shift_code or pd.isna(shift_code) or str(shift_code).strip() == '':
        return ''
    
    shift_code = str(shift_code).strip()
    
    # è™•ç†ç©ºå€¼å’Œç„¡æ•ˆå€¼
    if shift_code in ['nan', 'NaN', 'None', 'null', '']:
        return ''
    
    # æ˜ å°„è¡¨ï¼šå°‡å«æœ‰æ–œç·šçš„ç­åˆ¥è½‰æ›ç‚ºåŸºæœ¬ç­åˆ¥
    shift_mapping = {
        'FC/æ€¥æ•‘èª²': 'FC',
        'FC/ä¿é¤Š': 'FC', 
        'FC/æ—©ä¸Šå…¬å‡': 'FC',
        'FC/å·¥ç¨‹': 'FC',
        'FC/E1': 'FC',
        'FC/PM': 'FC',
        'P3n/å¤œè¶…': 'P3n',
        'P1n/å¤œè¶…': 'P1n',
        'P1n/LED': 'P1n',
        'P3c/ä¿é¤Š': 'P3c',
        'P1p/ä¿é¤Š': 'P1p',
        'P4p/ä¿é¤Š': 'P4p',
        'CH/FC': 'FC',
        'CH/FC*': 'FC'
    }
    
    # å…ˆæª¢æŸ¥ç›´æ¥æ˜ å°„
    if shift_code in shift_mapping:
        return shift_mapping[shift_code]
    
    # å¦‚æœåŒ…å«æ–œç·šï¼Œå–æ–œç·šå‰çš„éƒ¨åˆ†
    if '/' in shift_code:
        base_shift = shift_code.split('/')[0].strip()
        # é©—è­‰åŸºæœ¬ç­åˆ¥æ˜¯å¦æœ‰æ•ˆ
        valid_base_shifts = ['FC', 'P3n', 'P3c', 'P1p', 'P4p', 'P1n', 'P2n', 'P2p', 'P2s', 'P3p', 'P4n', 'CH']
        if base_shift in valid_base_shifts:
            return base_shift
    
    return shift_code

def validate_shift_count_equality(df):
    """é©—è­‰æ¯å€‹äººçš„ç­æ•¸æ˜¯å¦ç›¸ç­‰ - æŒ‰ç…§æ–°é‚è¼¯ï¼šæœ‰ç­å°±ç®—ï¼ŒåŒ…å«H0/H1"""
    validation_results = {
        'is_valid': True,
        'errors': [],
        'warnings': [],
        'statistics': {},
        'uneven_distribution': [],
        'duplicate_records': [],
        'part_time_employees': []
    }
    
    try:
        # çµ±è¨ˆæ¯å€‹äººçš„ç­æ•¸ - æœ‰ç­å°±ç®—ï¼ˆåŒ…å«H0, H1ï¼‰
        person_shift_counts = {}
        person_daily_shifts = {}  # è¨˜éŒ„æ¯å€‹äººæ¯å¤©çš„ç­åˆ¥
        
        # æ™ºèƒ½è­˜åˆ¥å§“åã€ç­åˆ¥å’Œæ—¥æœŸæ¬„ä½
        columns_info = identify_columns(df)
        name_col = columns_info.get('name_col')
        shift_col = columns_info.get('shift_col')
        
        # å˜—è©¦è­˜åˆ¥æ—¥æœŸæ¬„ä½
        date_col = None
        for col in df.columns:
            if 'æ—¥æœŸ' in str(col) or 'day' in str(col).lower():
                date_col = col
                break
        
        if not name_col or not shift_col:
            validation_results['is_valid'] = False
            validation_results['errors'].append('ç„¡æ³•è­˜åˆ¥å§“åæˆ–ç­åˆ¥æ¬„ä½')
            return validation_results
        
        # ç¬¬ä¸€éšæ®µï¼šæ”¶é›†æ‰€æœ‰ç­åˆ¥è³‡æ–™
        for index, row in df.iterrows():
            name = str(row[name_col]).strip()
            original_shift = str(row[shift_col]).strip()
            
            # è·³éæ¨™é¡Œè¡Œå’Œæ¨¡æ¿è¡Œ
            if name in ['å§“å', 'å¹´æœˆ', 'çµ„å®¤åˆ¥', 'å“¡å·¥ä»£ç¢¼', '', 'nan', 'NaN']:
                continue
            
            # è·³éç´”æ•¸å­—ï¼ˆæ—¥æœŸæ¨¡æ¿ï¼‰
            if name.isdigit():
                continue
            
            # è·³éé•·åº¦ç•°å¸¸çš„å§“å
            if len(name) < 2 or len(name) > 15:
                continue
            
            # ç²å–æ—¥æœŸè³‡è¨Š
            date_info = ''
            if date_col and pd.notna(row[date_col]):
                date_info = str(row[date_col]).strip()
            
            # è™•ç†ç­åˆ¥ï¼ˆæ¸…ç†æ›è¡Œç¬¦è™Ÿå’Œé¡å¤–è³‡è¨Šï¼‰
            shift = original_shift
            if shift:
                # æ¸…ç†æ›è¡Œç¬¦è™Ÿå’Œå¤šé¤˜ç©ºç™½
                shift = shift.replace('\n', '').replace('\r', '').strip()
                # å¦‚æœåŒ…å«æ™‚é–“è³‡è¨Šï¼ˆå¦‚ P4n\n13-22ï¼‰ï¼Œåªå–ç­åˆ¥éƒ¨åˆ†
                if '\n' in original_shift or any(char in shift for char in ['09-', '13-', '17-']):
                    # æå–ç­åˆ¥ä»£è™Ÿï¼ˆé€šå¸¸åœ¨æœ€å‰é¢ï¼‰
                    shift_parts = shift.split()
                    if shift_parts:
                        shift = shift_parts[0]
            
            # ç©ºç™½ç­åˆ¥è¦–ç‚ºéå…¨è·äººå“¡ï¼Œè¨˜éŒ„ä½†ä¸è¨ˆç®—ç­æ•¸
            if not shift or shift in ['', 'nan', 'NaN', 'None', 'null']:
                if name not in validation_results['part_time_employees']:
                    validation_results['part_time_employees'].append(name)
                continue
            
            # è¨˜éŒ„æ¯å€‹äººæ¯å¤©çš„ç­åˆ¥
            person_date_key = f"{name}_{date_info}"
            if person_date_key not in person_daily_shifts:
                person_daily_shifts[person_date_key] = {
                    'name': name,
                    'date': date_info,
                    'shifts': [],
                    'line': index + 1
                }
            person_daily_shifts[person_date_key]['shifts'].append(shift)
        
        # ç¬¬äºŒéšæ®µï¼šè™•ç†åˆä½µç­åˆ¥å’Œé‡è¤‡è¨˜éŒ„
        for person_date_key, daily_data in person_daily_shifts.items():
            name = daily_data['name']
            date = daily_data['date']
            shifts = daily_data['shifts']
            
            # å»é™¤é‡è¤‡çš„ç­åˆ¥ï¼Œä¿æŒé †åº
            unique_shifts = []
            for shift in shifts:
                if shift not in unique_shifts:
                    unique_shifts.append(shift)
            
            # å¦‚æœåŒä¸€å¤©æœ‰å¤šå€‹ä¸åŒç­åˆ¥ï¼Œåˆä½µç‚º ç­åˆ¥A/ç­åˆ¥B æ ¼å¼
            if len(shifts) > 1:
                if len(unique_shifts) > 1:
                    # ä¸åŒç­åˆ¥ï¼šé¡¯ç¤ºè­¦å‘Šä¸¦åˆä½µ
                    merged_shift = '/'.join(unique_shifts)
                    validation_results['warnings'].append(
                        f"{name} åœ¨ {date} æœ‰å¤šå€‹ç­åˆ¥ï¼š{merged_shift}"
                    )
                else:
                    # ç›¸åŒç­åˆ¥é‡è¤‡ï¼šé¡¯ç¤ºéŒ¯èª¤
                    validation_results['duplicate_records'].append({
                        'name': name,
                        'date': date,
                        'line': daily_data['line'],
                        'shift': unique_shifts[0],
                        'count': len(shifts)
                    })
            
            # çµ±è¨ˆç­æ•¸ï¼šæ¯å¤©ç®—ä¸€ç­ï¼Œä¸ç®¡æœ‰å¹¾å€‹ç­åˆ¥
            if name not in person_shift_counts:
                person_shift_counts[name] = 0
            person_shift_counts[name] += 1
        
        # åˆ†æç­æ•¸åˆ†å¸ƒ
        if person_shift_counts:
            shift_counts = list(person_shift_counts.values())
            min_shifts = min(shift_counts)
            max_shifts = max(shift_counts)
            avg_shifts = sum(shift_counts) / len(shift_counts)
            
            validation_results['statistics'] = {
                'total_people': len(person_shift_counts),
                'min_shifts': min_shifts,
                'max_shifts': max_shifts,
                'avg_shifts': round(avg_shifts, 2),
                'shift_difference': max_shifts - min_shifts
            }
            
            # æª¢æŸ¥ç­æ•¸å·®ç•°
            if max_shifts - min_shifts > 2:  # å…è¨±æœ€å¤š2ç­çš„å·®ç•°
                validation_results['is_valid'] = False
                validation_results['errors'].append(
                    f"ç­æ•¸åˆ†å¸ƒä¸å‡ï¼šæœ€å¤š{max_shifts}ç­ï¼Œæœ€å°‘{min_shifts}ç­ï¼Œå·®ç•°{max_shifts - min_shifts}ç­"
                )
            elif max_shifts - min_shifts > 0:
                validation_results['warnings'].append(
                    f"ç­æ•¸ç•¥æœ‰å·®ç•°ï¼šæœ€å¤š{max_shifts}ç­ï¼Œæœ€å°‘{min_shifts}ç­"
                )
            
            # æ‰¾å‡ºç­æ•¸ç•°å¸¸çš„äººå“¡
            target_shifts = round(avg_shifts)
            for name, count in person_shift_counts.items():
                if abs(count - target_shifts) > 1:
                    validation_results['uneven_distribution'].append({
                        'name': name,
                        'actual_shifts': count,
                        'expected_shifts': target_shifts,
                        'difference': count - target_shifts
                    })
        else:
            validation_results['warnings'].append('æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç­åˆ¥è³‡æ–™')
        
        # è™•ç†é‡è¤‡è¨˜éŒ„éŒ¯èª¤
        if validation_results['duplicate_records']:
            validation_results['is_valid'] = False
            for dup in validation_results['duplicate_records']:
                validation_results['errors'].append(
                    f"ç¬¬{dup['line']}è¡Œï¼šé‡è¤‡è¨˜éŒ„ - {dup['name']} åœ¨ {dup['date']} é‡è¤‡ {dup['count']} æ¬¡ç­åˆ¥ {dup['shift']}"
                )
        
        # è™•ç†éå…¨è·äººå“¡è³‡è¨Š
        if validation_results['part_time_employees']:
            validation_results['warnings'].append(
                f"ç™¼ç¾ {len(validation_results['part_time_employees'])} ä½éå…¨è·äººå“¡ï¼ˆæœ‰ç©ºç™½ç­åˆ¥ï¼‰"
            )
        
    except Exception as e:
        validation_results['is_valid'] = False
        validation_results['errors'].append(f"é©—è­‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    
    return validation_results

def validate_excel_data_v11(df, data_version, filename, target_group, whitelist_data):
    """v1.1ç‰ˆæœ¬çš„Excelè³‡æ–™é©—è­‰ - æ”¯æ´3æ¬„ä½å’Œ5æ¬„ä½æ ¼å¼ï¼Œå„ªåŒ–æ‰¹é‡åŒ¯å…¥"""
    result = {
        'status': 'OK',
        'total_records': len(df),
        'valid_records': 0,
        'warnings': 0,
        'errors': 0,
        'error_messages': [],
        'filename': filename,
        'data_version': data_version,
        'target_group': target_group,
        'batch_import_mode': True,  # æ¨™è¨˜ç‚ºæ‰¹é‡åŒ¯å…¥æ¨¡å¼
        'shift_equality': None  # æ–°å¢ç­æ•¸ç›¸ç­‰é©—è­‰çµæœ
    }
    
    try:
        # 1. æ¬„ä½é©—è­‰ - ä½¿ç”¨æ–°çš„æ™ºèƒ½è­˜åˆ¥ç³»çµ±
        columns_info = identify_columns(df)
        csv_format = columns_info.get('format', 'unknown')
        
        print(f"ğŸ” é©—è­‰è³‡æ–™ä½¿ç”¨æ ¼å¼: {csv_format}")
        
        name_col = columns_info.get('name_col')
        shift_col = columns_info.get('shift_col')
        
        if not name_col:
            result['errors'] += 1
            result['error_messages'].append('æ‰¾ä¸åˆ°å§“åæ¬„ä½ï¼ˆæ‡‰åŒ…å«ï¼šå§“åã€å“¡å·¥å§“åæˆ–nameï¼‰')
            result['status'] = 'ERROR'
        
        if not shift_col:
            result['errors'] += 1
            result['error_messages'].append('æ‰¾ä¸åˆ°ç­åˆ¥æ¬„ä½ï¼ˆæ‡‰åŒ…å«ï¼šç­åˆ¥ã€ç­æ¬¡æˆ–shiftï¼‰')
            result['status'] = 'ERROR'
        
        # æ ¹æ“šæ ¼å¼é©—è­‰æ—¥æœŸç›¸é—œæ¬„ä½
        if csv_format == '5_column':
            year_month_col = columns_info.get('year_month_col')
            day_col = columns_info.get('day_col')
            
            if not year_month_col:
                result['errors'] += 1
                result['error_messages'].append('5æ¬„ä½æ ¼å¼ä¸­æ‰¾ä¸åˆ°å¹´æœˆæ¬„ä½ï¼ˆæ‡‰åŒ…å«ï¼šå¹´æœˆï¼‰')
                result['status'] = 'ERROR'
            
            if not day_col:
                result['errors'] += 1
                result['error_messages'].append('5æ¬„ä½æ ¼å¼ä¸­æ‰¾ä¸åˆ°æ—¥æœŸæ¬„ä½ï¼ˆæ‡‰åŒ…å«ï¼šæ—¥æœŸã€æ—¥æˆ–dayï¼‰')
                result['status'] = 'ERROR'
        else:
            date_col = columns_info.get('date_col')
            if not date_col:
                result['errors'] += 1
                result['error_messages'].append('æ‰¾ä¸åˆ°æ—¥æœŸæ¬„ä½ï¼ˆæ‡‰åŒ…å«ï¼šæ—¥æœŸæˆ–dateï¼‰')
                result['status'] = 'ERROR'
        
        # å¦‚æœåŸºæœ¬æ¬„ä½éƒ½æ‰¾ä¸åˆ°ï¼Œç›´æ¥è¿”å›
        if result['status'] == 'ERROR':
            return result
        
        # 2. ç²å–ç™½åå–®å’Œæœ‰æ•ˆç­åˆ¥
        if target_group == 'å…¨åå–®':
            # çœŸæ­£çš„å…¨åå–®ï¼šä¸ä½¿ç”¨ç™½åå–®é™åˆ¶ï¼Œå¾CSVä¸­æå–æ‰€æœ‰å”¯ä¸€å§“å
            if name_col:
                valid_names = df[name_col].dropna().astype(str).str.strip().unique().tolist()
                print(f"ğŸ”§ [èª¿è©¦] é©—è­‰éšæ®µ-å…¨åå–®æ¨¡å¼ï¼šå¾CSVæå–åˆ° {len(valid_names)} å€‹å”¯ä¸€å§“å")
            else:
                valid_names = []
        else:
            valid_names = whitelist_data.get('group', {}).get(target_group, [])
        
        valid_shifts = get_valid_shift_codes()
        
        # 3. è³‡æ–™å…§å®¹é©—è­‰ - æ‰¹é‡è™•ç†æ¨¡å¼
        daily_schedules = {}  # ç”¨æ–¼æª¢æŸ¥é‡è¤‡å’Œå–®æ—¥äººæ•¸
        batch_errors = []  # æ”¶é›†æ‰¹é‡éŒ¯èª¤
        processed_count = 0
        
        print(f"ğŸš€ é–‹å§‹æ‰¹é‡é©—è­‰ {len(df)} ç­†è¨˜éŒ„...")
        
        for index, row in df.iterrows():
            processed_count += 1
            
            # æ¯100ç­†é¡¯ç¤ºé€²åº¦
            if processed_count % 100 == 0:
                print(f"ğŸ“Š å·²è™•ç† {processed_count}/{len(df)} ç­†è¨˜éŒ„")
            
            try:
                # å§“åé©—è­‰
                employee_name = str(row[name_col]).strip() if pd.notna(row[name_col]) else ''
                if not employee_name:
                    result['errors'] += 1
                    result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šå§“åæ¬„ä½ç©ºç™½')
                    continue
                
                # ç¾¤çµ„éæ¿¾ï¼šåªè™•ç†å±¬æ–¼ç›®æ¨™ç¾¤çµ„çš„å“¡å·¥ï¼Œè·³éå…¶ä»–å“¡å·¥
                if employee_name not in valid_names:
                    # è·³éä¸åœ¨ç›®æ¨™ç¾¤çµ„ä¸­çš„å“¡å·¥ï¼Œä¸é¡¯ç¤ºéŒ¯èª¤è¨Šæ¯
                    continue
                
                # ä½¿ç”¨å¢å¼·çš„æ—¥æœŸç²å–åŠŸèƒ½
                date_value = get_date_value_enhanced(row, columns_info)
                if not date_value:
                    result['errors'] += 1
                    result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šæ—¥æœŸæ¬„ä½ç©ºç™½æˆ–ç„¡æ³•è§£æ')
                    continue
                
                # å˜—è©¦è§£ææ—¥æœŸ
                try:
                    if isinstance(date_value, str):
                        schedule_date = datetime.strptime(date_value, '%Y-%m-%d').date()
                    elif hasattr(date_value, 'strftime'):
                        # å¦‚æœå·²ç¶“æ˜¯dateå°è±¡
                        schedule_date = date_value
                    else:
                        schedule_date = date_value.date() if hasattr(date_value, 'date') else date_value
                except Exception as date_error:
                    result['errors'] += 1
                    result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šæ—¥æœŸæ¬„éæ—¥æœŸæ ¼å¼')
                    continue
                
                # ç­åˆ¥é©—è­‰ - å…ˆæ¨™æº–åŒ–ç­åˆ¥ä»£è™Ÿ
                original_shift = str(row[shift_col]).strip() if pd.notna(row[shift_col]) else ''
                if not original_shift:
                    result['errors'] += 1
                    result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šç­åˆ¥æ¬„ä½ç©ºç™½')
                    continue
                
                # æ¨™æº–åŒ–ç­åˆ¥ä»£è™Ÿ
                shift_code = normalize_shift_code(original_shift)
                
                if shift_code not in valid_shifts:
                    result['errors'] += 1
                    result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šç­åˆ¥ä»£è™ŸéŒ¯èª¤ï¼š{original_shift} (æ¨™æº–åŒ–å¾Œ: {shift_code})')
                    continue
                
                # å¦‚æœæœ‰è½‰æ›ï¼Œè¨˜éŒ„è½‰æ›è³‡è¨Š
                if original_shift != shift_code:
                    result['warnings'] += 1
                    result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šç­åˆ¥å·²è‡ªå‹•è½‰æ›ï¼š{original_shift} â†’ {shift_code}')
                
                # é‡è¤‡è³‡æ–™æª¢æŸ¥
                schedule_key = f"{employee_name}_{schedule_date}"
                if schedule_key in daily_schedules:
                    result['warnings'] += 1
                    result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šé‡è¤‡è³‡æ–™ - {employee_name} åœ¨ {schedule_date} å·²æœ‰æ’ç­')
                    if result['status'] == 'OK':
                        result['status'] = 'WARNING'
                else:
                    daily_schedules[schedule_key] = True
                    
                    # çµ±è¨ˆæ¯æ—¥äººæ•¸
                    if schedule_date not in daily_schedules:
                        daily_schedules[schedule_date] = []
                    daily_schedules[schedule_date].append(employee_name)
                
                result['valid_records'] += 1
                
            except Exception as e:
                result['errors'] += 1
                result['error_messages'].append(f'ç¬¬{index+2}è¡Œï¼šè³‡æ–™è™•ç†éŒ¯èª¤ - {str(e)}')
        
        # 4. æª¢æŸ¥å–®æ—¥äººæ•¸è­¦å‘Š
        for schedule_date, employees in daily_schedules.items():
            if isinstance(employees, list) and len(employees) == 1:
                result['warnings'] += 1
                result['error_messages'].append(f'{schedule_date}ï¼šç•¶æ—¥åƒ…ä¸€äººä¸Šç­ï¼Œè«‹æ³¨æ„è£œäººæ‰‹')
                if result['status'] == 'OK':
                    result['status'] = 'WARNING'
        
        # 5. æª¢æŸ¥ç‡ˆå…‰çµ„æ ¸å¿ƒ7äººç­æ•¸å¹³è¡¡ï¼ˆæ”¯æ´ç›´å¼å’Œæ©«å¼æ ¼å¼ï¼‰
        core_light_crew = ['è³´ ç§‰ å®', 'æ æƒŸ ç¶±', 'æ å®¶ ç‘‹', 'ç‹ å¿— å¿ ', 'é¡§ è‚² ç¦', 'èƒ¡ ç¿Š æ½”', 'æœ± å®¶ å¾·']
        core_crew_stats = {}
        
        # åˆ¤æ–·æ˜¯ç›´å¼é‚„æ˜¯æ©«å¼æ ¼å¼
        is_vertical_format = (name_col and shift_col and 
                             'å§“å' in str(name_col).lower() and 
                             ('ç­åˆ¥' in str(shift_col).lower() or 'shift' in str(shift_col).lower()))
        
        if is_vertical_format:
            # ç›´å¼æ ¼å¼çµ±è¨ˆï¼šæ¯è¡Œä»£è¡¨ä¸€å€‹äººä¸€å¤©çš„ç­åˆ¥
            for index, row in df.iterrows():
                try:
                    # ç²å–å§“å
                    employee_name = str(row[name_col]).strip() if pd.notna(row[name_col]) else ''
                    
                    # è·³éæ¨™é¡Œè¡Œ
                    if employee_name in ['å§“å', 'å¹´æœˆ', 'çµ„å®¤åˆ¥', 'å“¡å·¥ä»£ç¢¼']:
                        continue
                    
                    if employee_name in core_light_crew:
                        if employee_name not in core_crew_stats:
                            core_crew_stats[employee_name] = 0
                        
                        # ç²å–ç­åˆ¥
                        shift_value = str(row[shift_col]).strip() if pd.notna(row[shift_col]) else ''
                        
                        # è¨ˆç®—ç­æ•¸ï¼šæ’é™¤ä¼‘å‡ã€èˆå°ç­å’Œç©ºå€¼
                        if shift_value and shift_value not in ['OFF', 'nan', '', 'ä¼‘å‡', 'èˆå°']:
                            core_crew_stats[employee_name] += 1
                            
                except Exception as e:
                    print(f"çµ±è¨ˆæ ¸å¿ƒäººå“¡æ™‚å‡ºéŒ¯ (ç›´å¼): {e}")
                    continue
        else:
            # æ©«å¼æ ¼å¼çµ±è¨ˆï¼šå‚³çµ±æ–¹å¼
            numeric_cols = [col for col in df.columns if str(col).strip().isdigit()]
            
            for index, row in df.iterrows():
                try:
                    # å˜—è©¦å¤šç¨®æ–¹å¼æ‰¾åˆ°å§“å
                    employee_name = ''
                    
                    # æ–¹å¼1: ä½¿ç”¨è­˜åˆ¥åˆ°çš„å§“åæ¬„ä½
                    if name_col and name_col in df.columns:
                        employee_name = str(row[name_col]).strip() if pd.notna(row[name_col]) else ''
                    
                    # æ–¹å¼2: æª¢æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦æœ‰å§“åè³‡è¨Š
                    if not employee_name and len(df) > 0:
                        first_row = df.iloc[0]
                        for col_idx, header_value in enumerate(first_row):
                            if str(header_value).strip() == 'å§“å' and col_idx < len(row):
                                employee_name = str(row.iloc[col_idx]).strip() if pd.notna(row.iloc[col_idx]) else ''
                                break
                    
                    # æ–¹å¼3: å˜—è©¦ç¬¬ä¸€æ¬„
                    if not employee_name:
                        employee_name = str(row.iloc[0]).strip() if len(row) > 0 and pd.notna(row.iloc[0]) else ''
                    
                    # è·³éæ¨™é¡Œè¡Œ
                    if employee_name in ['å§“å', 'å¹´æœˆ', 'çµ„å®¤åˆ¥', 'å“¡å·¥ä»£ç¢¼']:
                        continue
                    
                    if employee_name in core_light_crew:
                        if employee_name not in core_crew_stats:
                            core_crew_stats[employee_name] = 0
                        
                        # çµ±è¨ˆè©²å“¡å·¥åœ¨æ‰€æœ‰æ—¥æœŸæ¬„ä½ä¸­çš„ç­åˆ¥æ•¸
                        for day_col in numeric_cols:
                            if day_col in row.index:
                                shift_value = str(row[day_col]).strip() if pd.notna(row[day_col]) else ''
                                if shift_value and shift_value not in ['OFF', 'nan', '', 'ä¼‘å‡', 'èˆå°']:
                                    core_crew_stats[employee_name] += 1
                        
                except Exception as e:
                    print(f"çµ±è¨ˆæ ¸å¿ƒäººå“¡æ™‚å‡ºéŒ¯ (æ©«å¼): {e}")
                    continue
        
        # å°‡çµ±è¨ˆçµæœåŠ å…¥é©—è­‰çµæœ
        result['core_light_crew_stats'] = core_crew_stats
        
        # æª¢æŸ¥ç­æ•¸æ˜¯å¦å¹³è¡¡
        if core_crew_stats:
            shift_counts = list(core_crew_stats.values())
            if shift_counts and len(set(shift_counts)) > 1:
                min_count = min(shift_counts)
                max_count = max(shift_counts)
                result['warnings'] += 1
                result['error_messages'].append(f'ç‡ˆå…‰çµ„æ ¸å¿ƒäººå“¡ç­æ•¸ä¸å¹³è¡¡ï¼šæœ€å°‘{min_count}ç­ï¼Œæœ€å¤š{max_count}ç­')
                if result['status'] == 'OK':
                    result['status'] = 'WARNING'
                
                # è©³ç´°åˆ—å‡ºæ¯å€‹äººçš„ç­æ•¸
                stats_details = []
                for name, count in core_crew_stats.items():
                    stats_details.append(f'{name}: {count}ç­')
                result['error_messages'].append(f'æ ¸å¿ƒäººå“¡ç­æ•¸æ˜ç´°ï¼š{", ".join(stats_details)}')
        
        # 6. ç­æ•¸ç›¸ç­‰é©—è­‰
        shift_equality = validate_shift_count_equality(df)
        result['shift_equality'] = shift_equality
        
        # æ•´åˆç­æ•¸é©—è­‰çµæœ
        if not shift_equality['is_valid']:
            result['errors'] += len(shift_equality['errors'])
            result['error_messages'].extend(shift_equality['errors'])
            result['status'] = 'ERROR'
        
        if shift_equality['warnings']:
            result['warnings'] += len(shift_equality['warnings'])
            result['error_messages'].extend(shift_equality['warnings'])
            if result['status'] == 'OK':
                result['status'] = 'WARNING'
        
        # 7. è¨­å®šæœ€çµ‚ç‹€æ…‹
        if result['errors'] > 0:
            result['status'] = 'ERROR'
        elif result['warnings'] > 0:
            result['status'] = 'WARNING'
        
        return result
        
    except Exception as e:
        result['status'] = 'ERROR'
        result['errors'] += 1
        result['error_messages'].append(f'é©—è­‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}')
        return result

def get_target_employees():
    """ç²å–ç›®æ¨™å“¡å·¥åå–®ï¼ˆä¿ç•™å‘å¾Œå…¼å®¹æ€§ï¼‰"""
    return [
        'è³´ç§‰å®', 'ææƒŸç¶±', 'æå®¶ç‘‹', 'ç‹å¿—å¿ ', 'é¡§è‚²ç¦',
        'èƒ¡ç¿Šæ½”', 'æœ±å®¶å¾·', 'é™³éŸ‹å¦‚', 'è‘›ç¦', 'äº•åº·ç¾½',
        'ç°¡èŠ³ç‘œ', 'æ¢å¼˜å²³', 'æä½©ç’‡', 'é„­æ ¢ç”', 'ç‹æ–‡æ€¡'
    ]

@main.route('/confirm_import', methods=['POST'])
@require_admin
def confirm_import():
    """ç¢ºèªä¸¦åŸ·è¡ŒåŒ¯å…¥ï¼ˆç„¡éœ€sessionæš«å­˜ï¼‰"""
    try:
        csv_data = request.form.get('csv_data')
        target_group = request.form.get('target_group')
        filename = request.form.get('filename')
        force_import = request.form.get('force_import') == 'true'
        allow_blank_shifts = request.form.get('allow_blank_shifts') == 'true'
        import_mode = request.form.get('import_mode', 'merge')  # é»˜èªç‚ºåˆä½µæ¨¡å¼
        
        if not csv_data or not target_group or not filename:
            flash('åŒ¯å…¥è³‡æ–™ä¸å®Œæ•´ï¼Œè«‹é‡æ–°ä¸Šå‚³', 'error')
            return redirect(url_for('main.upload_new'))
        
        # è§£æCSVè³‡æ–™
        import io
        import base64
        csv_string = base64.b64decode(csv_data).decode('utf-8')
        df = pd.read_csv(io.StringIO(csv_string))
        
        # é‡æ–°é©—è­‰è³‡æ–™
        whitelist_data = load_whitelist()
        validation_result = validate_excel_data_v11(df, 'ä¸€èˆ¬ç‰ˆæœ¬', filename, target_group, whitelist_data)
        
        # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŒ¯å…¥
        if validation_result['status'] == 'ERROR' and not force_import:
            flash('è³‡æ–™æœ‰éŒ¯èª¤ï¼Œç„¡æ³•åŒ¯å…¥', 'error')
            return redirect(url_for('main.upload_new'))
        
        # åŸ·è¡ŒåŒ¯å…¥
        import_options = {
            'force_import': force_import,
            'allow_blank_shifts': allow_blank_shifts,
            'import_mode': import_mode
        }
        import_result = perform_data_import_v11(df, validation_result, target_group, False, import_mode, import_options)
        
        # è¨˜éŒ„åŒ¯å…¥æ—¥èªŒ
        current_user = get_current_user()
        import_log = ImportLog(
            importer=current_user.name,
            filename=filename,
            data_version='ä¸€èˆ¬ç‰ˆæœ¬',
            target_group=target_group,
            validation_result=validation_result['status'],
            force_import=force_import,
            error_count=validation_result['errors'],
            warning_count=validation_result['warnings'],
            records_imported=import_result.get('imported_count', 0) + import_result.get('updated_count', 0)
        )
        
        if validation_result['error_messages']:
            import_log.set_validation_errors(validation_result['error_messages'])
        
        db.session.add(import_log)
        db.session.commit()
        
        # é¡¯ç¤ºçµæœè¨Šæ¯
        if import_result['success']:
            imported = import_result.get('imported_count', 0)
            updated = import_result.get('updated_count', 0) 
            skipped = import_result.get('skipped_count', 0)
            mode_text = "åˆä½µæ¨¡å¼" if import_mode == 'merge' else "è¦†å¯«æ¨¡å¼"
            flash(f'âœ… åŒ¯å…¥æˆåŠŸï¼({mode_text}) æ–°å¢: {imported}ç­†, æ›´æ–°: {updated}ç­†, è·³é: {skipped}ç­†', 'success')
        else:
            flash(f'âŒ åŒ¯å…¥å¤±æ•—ï¼š{import_result["error"]}', 'error')
        
        return redirect(url_for('main.upload_new'))
        
    except Exception as e:
        print(f"åŒ¯å…¥éŒ¯èª¤: {e}")
        flash(f'åŒ¯å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}', 'error')
        return redirect(url_for('main.upload_new'))

@main.route('/execute_import', methods=['POST'])
@require_admin
def execute_import():
    """åŸ·è¡ŒåŒ¯å…¥ä½œæ¥­"""
    try:
        data_hash = request.form.get('validated_data')
        force_import = request.form.get('force_import') == 'true'
        
        if not data_hash or f'validated_data_{data_hash}' not in session:
            flash('é©—è­‰è³‡æ–™å·²éæœŸï¼Œè«‹é‡æ–°ä¸Šå‚³', 'error')
            return redirect(url_for('main.upload_new'))
        
        cached_data = session[f'validated_data_{data_hash}']
        df = pd.read_json(cached_data['df'])
        validation_result = cached_data['validation_result']
        filename = cached_data['filename']
        data_version = cached_data['data_version']
        target_group = cached_data.get('target_group', 'å…¨åå–®')
        skip_invalid = cached_data.get('skip_invalid', False)
        
        # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŒ¯å…¥
        if validation_result['status'] == 'ERROR' and not force_import:
            flash('è³‡æ–™æœ‰éŒ¯èª¤ï¼Œç„¡æ³•åŒ¯å…¥', 'error')
            return redirect(url_for('main.upload_new'))
        
        # åŸ·è¡ŒåŒ¯å…¥
        import_result = perform_data_import_v11(df, validation_result, target_group, skip_invalid)
        
        # è¨˜éŒ„åŒ¯å…¥æ—¥èªŒ
        current_user = get_current_user()
        import_log = ImportLog(
            importer=current_user.name,
            filename=filename,
            data_version=data_version,
            target_group=target_group,
            validation_result=validation_result['status'],
            force_import=force_import,
            error_count=validation_result['errors'],
            warning_count=validation_result['warnings'],
            records_imported=import_result['imported_count']
        )
        
        if validation_result['error_messages']:
            import_log.set_validation_errors(validation_result['error_messages'])
        
        db.session.add(import_log)
        db.session.commit()
        
        # æ¸…ç†ç·©å­˜
        del session[f'validated_data_{data_hash}']
        
        success_message = f'åŒ¯å…¥å®Œæˆï¼æˆåŠŸåŒ¯å…¥ {import_result["imported_count"]} ç­†è¨˜éŒ„'
        if import_result['skipped_count'] > 0:
            success_message += f'ï¼Œè·³é {import_result["skipped_count"]} ç­†é‡è¤‡è¨˜éŒ„'
        
        flash(success_message, 'success')
        return redirect(url_for('main.upload_new'))
        
    except Exception as e:
        db.session.rollback()
        flash(f'åŒ¯å…¥å¤±æ•—: {str(e)}', 'error')
        return redirect(url_for('main.upload_new'))

@main.route('/api/employee-groups', methods=['GET'])
@require_auth
def get_employee_groups():
    """ç²å–å“¡å·¥ç¾¤çµ„æ­¸å±¬è³‡æ–™"""
    try:
        # ç²å–æ‰€æœ‰ç¾¤çµ„è³‡æ–™
        all_groups = GroupMembers.query.all()
        employee_groups = {}
        
        for group in all_groups:
            members = group.get_members()
            for member_name in members:
                if member_name not in employee_groups:
                    employee_groups[member_name] = []
                employee_groups[member_name].append(group.group_name)
        
        return jsonify({
            'success': True,
            'data': employee_groups
        })
    except Exception as e:
        print(f'ç²å–å“¡å·¥ç¾¤çµ„éŒ¯èª¤: {e}')
        return jsonify({'success': False, 'message': 'ç²å–ç¾¤çµ„è³‡æ–™å¤±æ•—'}), 500

@main.route('/api/group-members', methods=['GET'])
@require_auth
def get_group_members():
    """ç²å–æ‰€æœ‰ç¾¤çµ„çš„äººå“¡åå–®"""
    try:
        groups = GroupMembers.query.all()
        group_data = {}
        
        # é è¨­ç¾¤çµ„
        default_groups = ['çµ±ç±Œçµ„', 'ç‡ˆå…‰çµ„', 'èˆå°çµ„', 'è¦–è½çµ„', 'ç¶­è­·çµ„']
        
        for group_name in default_groups:
            group_data[group_name] = []
        
        # è¼‰å…¥è³‡æ–™åº«ä¸­çš„ç¾¤çµ„è³‡æ–™
        for group in groups:
            group_data[group.group_name] = group.get_members()
        
        return jsonify({
            'status': 'success',
            'data': group_data
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@main.route('/api/group-members', methods=['POST'])
@require_admin
def save_group_members():
    """å„²å­˜ç¾¤çµ„äººå“¡è¨­å®š"""
    try:
        data = request.get_json()
        
        for group_name, members in data.items():
            # æŸ¥æ‰¾æˆ–å‰µå»ºç¾¤çµ„è¨˜éŒ„
            group = GroupMembers.query.filter_by(group_name=group_name).first()
            if not group:
                group = GroupMembers(group_name=group_name)
                db.session.add(group)
            
            # æ¸…ç†å’Œè¨­ç½®æˆå“¡åå–®
            cleaned_members = [name.strip() for name in members if name.strip()]
            group.set_members(cleaned_members)
        
        db.session.commit()
        
        return jsonify({
            'status': 'success',
            'message': 'ç¾¤çµ„è¨­å®šå·²å„²å­˜'
        })
    except Exception as e:
        db.session.rollback()
        return jsonify({
            'status': 'error',
            'message': f'å„²å­˜å¤±æ•—: {str(e)}'
        }), 500

@main.route('/api/group-members/<group_name>', methods=['PUT'])
@require_admin
def update_group_members(group_name):
    """æ›´æ–°ç‰¹å®šç¾¤çµ„çš„äººå“¡åå–®"""
    try:
        data = request.get_json()
        members = data.get('members', [])
        
        # æŸ¥æ‰¾æˆ–å‰µå»ºç¾¤çµ„è¨˜éŒ„
        group = GroupMembers.query.filter_by(group_name=group_name).first()
        if not group:
            group = GroupMembers(group_name=group_name)
            db.session.add(group)
        
        # æ¸…ç†å’Œè¨­ç½®æˆå“¡åå–®
        cleaned_members = [name.strip() for name in members if name.strip()]
        group.set_members(cleaned_members)
        
        db.session.commit()
        
        return jsonify({
            'status': 'success',
            'message': f'{group_name} äººå“¡åå–®å·²æ›´æ–°',
            'count': len(cleaned_members)
        })
    except Exception as e:
        db.session.rollback()
        return jsonify({
            'status': 'error',
            'message': f'æ›´æ–°å¤±æ•—: {str(e)}'
        }), 500

@main.route('/api/clear-all-data', methods=['POST'])
@require_admin
def clear_all_data():
    """æ¸…é™¤æ‰€æœ‰ç­è¡¨è³‡æ–™å’ŒåŒ¯å…¥è¨˜éŒ„ï¼ˆä¿ç•™å“¡å·¥å’Œç­åˆ¥è¨­å®šï¼‰"""
    try:
        # è¨ˆç®—æ¸…é™¤å‰çš„è³‡æ–™çµ±è¨ˆ
        schedule_count = Schedule.query.count()
        import_log_count = ImportLog.query.count()
        
        print(f"ğŸ—‘ï¸ æº–å‚™æ¸…é™¤è³‡æ–™ï¼š{schedule_count} ç­†æ’ç­è¨˜éŒ„ï¼Œ{import_log_count} ç­†åŒ¯å…¥è¨˜éŒ„")
        
        # æ¸…é™¤æ’ç­è¨˜éŒ„
        Schedule.query.delete()
        
        # æ¸…é™¤åŒ¯å…¥æ—¥èªŒ
        ImportLog.query.delete()
        
        # æäº¤è®Šæ›´
        db.session.commit()
        
        message = f"æˆåŠŸæ¸…é™¤ {schedule_count} ç­†æ’ç­è¨˜éŒ„å’Œ {import_log_count} ç­†åŒ¯å…¥è¨˜éŒ„"
        print(f"âœ… {message}")
        
        return jsonify({
            'status': 'success',
            'message': message,
            'cleared': {
                'schedules': schedule_count,
                'import_logs': import_log_count
            }
        })
    except Exception as e:
        db.session.rollback()
        print(f"âŒ æ¸…é™¤è³‡æ–™å¤±æ•—: {str(e)}")
        return jsonify({
            'status': 'error',
            'message': f'æ¸…é™¤å¤±æ•—: {str(e)}'
        }), 500

def perform_data_import_v11(df, validation_result, target_group, skip_invalid, import_mode='merge', import_options=None):
    """åŸ·è¡Œv1.1ç‰ˆæœ¬çš„å¯¦éš›è³‡æ–™åŒ¯å…¥ - æ”¯æ´3æ¬„ä½å’Œ5æ¬„ä½æ ¼å¼ï¼Œå„ªåŒ–æ‰¹é‡è™•ç†ï¼Œæ–°å¢å¼·åˆ¶åŒ¯å…¥åŠŸèƒ½"""
    if import_options is None:
        import_options = {}
    
    force_import = import_options.get('force_import', False)
    allow_blank_shifts = import_options.get('allow_blank_shifts', True)
    imported_count = 0
    skipped_count = 0
    updated_count = 0
    
    try:
        # è¼‰å…¥ç™½åå–®
        whitelist_data = load_whitelist()
        columns_info = identify_columns(df)
        
        name_col = columns_info.get('name_col')
        shift_col = columns_info.get('shift_col')
        
        print(f"ğŸš€ åŒ¯å…¥ä½¿ç”¨æ ¼å¼: {columns_info.get('format', 'unknown')}")
        print(f"ğŸ“‹ åŒ¯å…¥æ¨¡å¼: {import_mode}")
        print(f"ğŸ“Š æº–å‚™è™•ç† {len(df)} ç­†è¨˜éŒ„...")
        
        # å¦‚æœæ˜¯è¦†è“‹æ¨¡å¼ï¼Œéœ€è¦æ¸…é™¤ç¾æœ‰è³‡æ–™
        if import_mode == 'overwrite':
            print("ğŸ—‘ï¸ è¦†è“‹æ¨¡å¼ï¼šæ¸…é™¤ç¾æœ‰æ’ç­è³‡æ–™...")
            # åªæ¸…é™¤ç›®æ¨™ç¾¤çµ„çš„è³‡æ–™ï¼Œä¸æ˜¯å…¨éƒ¨æ¸…é™¤
            if target_group != 'å…¨åå–®':
                # æ¸…é™¤ç‰¹å®šç¾¤çµ„çš„æ’ç­è³‡æ–™
                pass  # é€™è£¡å¯ä»¥æ·»åŠ æ¸…é™¤é‚è¼¯
        
        if target_group == 'å…¨åå–®':
            # å¯¦éš›åŒ¯å…¥éšæ®µï¼šå…¨åå–®æ¨¡å¼ä¸é™åˆ¶å§“å
            valid_names = df[name_col].dropna().astype(str).str.strip().unique().tolist()
            print(f"ğŸ”§ [èª¿è©¦] åŒ¯å…¥éšæ®µ-å…¨åå–®æ¨¡å¼ï¼šè™•ç† {len(valid_names)} å€‹å”¯ä¸€å§“å")
        else:
            valid_names = whitelist_data.get('group', {}).get(target_group, [])
        
        valid_shifts = get_valid_shift_codes()
        
        processed_count = 0
        batch_size = 100  # æ¯100ç­†æäº¤ä¸€æ¬¡
        
        print(f"ğŸš€ é–‹å§‹æ‰¹é‡åŒ¯å…¥ï¼Œæ¯ {batch_size} ç­†æäº¤ä¸€æ¬¡...")
        
        for index, row in df.iterrows():
            processed_count += 1
            
            # æ¯100ç­†é¡¯ç¤ºé€²åº¦
            if processed_count % batch_size == 0:
                print(f"ğŸ“ˆ å·²è™•ç† {processed_count}/{len(df)} ç­†è¨˜éŒ„ (åŒ¯å…¥: {imported_count}, æ›´æ–°: {updated_count}, è·³é: {skipped_count})")
                # æ‰¹é‡æäº¤åˆ°è³‡æ–™åº«
                db.session.commit()
            try:
                # æå–å§“åå’Œç­åˆ¥
                employee_name = str(row[name_col]).strip()
                
                # è™•ç†ç­åˆ¥æ¬„ä½ï¼ˆæ”¯æ´ç©ºç™½å’Œè¤‡åˆæ ¼å¼ï¼‰
                shift_raw = row[shift_col] if pd.notna(row[shift_col]) else ''
                shift_code = str(shift_raw).strip()
                
                # è™•ç†ç©ºç™½ç­åˆ¥
                if not shift_code:
                    if allow_blank_shifts:
                        shift_code = 'UNASSIGNED'  # æœªæ’ç­ä»£ç¢¼
                        print(f"ğŸ“ ç©ºç™½ç­åˆ¥è™•ç†ç‚ºæœªæ’ç­: {employee_name}")
                    else:
                        skipped_count += 1
                        continue
                
                # è™•ç†è¤‡åˆç­åˆ¥æ ¼å¼ï¼ˆå¦‚ P1c/å·¥ç¨‹, P4p/ME ç­‰ï¼‰
                original_shift_code = shift_code
                if '/' in shift_code:
                    # æå–ä¸»è¦ç­åˆ¥ä»£ç¢¼ï¼ˆ/ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
                    base_shift = shift_code.split('/')[0].strip()
                    suffix = shift_code.split('/')[1].strip() if len(shift_code.split('/')) > 1 else ''
                    print(f"ğŸ”§ è¤‡åˆç­åˆ¥: {shift_code} â†’ åŸºç¤ç­åˆ¥: {base_shift}, å¾Œç¶´: {suffix}")
                    
                    if force_import:
                        # å¼·åˆ¶åŒ¯å…¥æ¨¡å¼ï¼šä¿æŒå®Œæ•´çš„è¤‡åˆç­åˆ¥ä»£ç¢¼
                        pass
                    else:
                        # ä¸€èˆ¬æ¨¡å¼ï¼šä½¿ç”¨åŸºç¤ç­åˆ¥ä»£ç¢¼é€²è¡Œé©—è­‰
                        shift_code = base_shift
                
                # è™•ç†ç‰¹æ®Šæ ¼å¼ï¼ˆå¦‚å¸¶æ›è¡Œçš„ç­åˆ¥ï¼‰
                if '\n' in shift_code:
                    shift_code = shift_code.replace('\n', ' ').strip()
                
                # æå–å“¡å·¥ä»£ç¢¼ï¼ˆå¦‚æœæ˜¯5æ¬„ä½æ ¼å¼ï¼‰
                employee_code = None
                if columns_info.get('format') == '5_column' and columns_info.get('employee_code_col'):
                    employee_code = str(row[columns_info['employee_code_col']]).strip() if pd.notna(row[columns_info['employee_code_col']]) else None
                
                # ä½¿ç”¨å¢å¼·çš„æ—¥æœŸç²å–åŠŸèƒ½
                date_value = get_date_value_enhanced(row, columns_info)
                
                # è·³éç©ºç™½æˆ–ç„¡æ•ˆè³‡æ–™
                if not employee_name or not date_value:
                    continue
                
                # ç¾¤çµ„éæ¿¾æª¢æŸ¥ï¼šå¦‚æœä¸æ˜¯å…¨åå–®æ¨¡å¼ï¼Œåªè™•ç†ç›®æ¨™ç¾¤çµ„çš„å“¡å·¥
                if target_group != 'å…¨åå–®' and employee_name not in valid_names:
                    skipped_count += 1
                    continue
                
                # ç­åˆ¥ä»£ç¢¼æª¢æŸ¥ï¼ˆå¼·åˆ¶åŒ¯å…¥æ¨¡å¼æœƒè‡ªå‹•å‰µå»ºæ–°ç­åˆ¥ï¼‰
                if shift_code not in valid_shifts and not force_import:
                    print(f"âš ï¸ æœªçŸ¥ç­åˆ¥ä»£ç¢¼: {shift_code} (ç¬¬{index+2}è¡Œ)")
                    skipped_count += 1
                    continue
                
                # è™•ç†æ—¥æœŸ
                try:
                    if isinstance(date_value, str):
                        schedule_date = datetime.strptime(date_value, '%Y-%m-%d').date()
                    elif hasattr(date_value, 'strftime'):
                        # å¦‚æœå·²ç¶“æ˜¯dateå°è±¡
                        schedule_date = date_value
                    else:
                        schedule_date = date_value.date() if hasattr(date_value, 'date') else date_value
                except Exception as date_error:
                    print(f"âŒ æ—¥æœŸè§£æå¤±æ•— (ç¬¬{index+2}è¡Œ): {date_value}, éŒ¯èª¤: {date_error}")
                    skipped_count += 1
                    continue
                
                # æŸ¥æ‰¾æˆ–å‰µå»ºå“¡å·¥
                employee = Employee.query.filter_by(name=employee_name).first()
                if not employee:
                    # å„ªå…ˆä½¿ç”¨CSVä¸­çš„å“¡å·¥ä»£ç¢¼ï¼Œå¦å‰‡è‡ªå‹•ç”Ÿæˆ
                    final_employee_code = employee_code
                    if employee_code and employee_code.strip():
                        # æª¢æŸ¥å“¡å·¥ä»£ç¢¼æ˜¯å¦å·²è¢«ä½¿ç”¨
                        if Employee.query.filter_by(employee_code=employee_code).first():
                            print(f"âš ï¸ å“¡å·¥ä»£ç¢¼ {employee_code} å·²å­˜åœ¨ï¼Œå°‡è‡ªå‹•ç”Ÿæˆæ–°ä»£ç¢¼")
                            final_employee_code = None
                    
                    if not final_employee_code:
                        # è‡ªå‹•ç”Ÿæˆå“¡å·¥ä»£ç¢¼
                        existing_count = Employee.query.count()
                        final_employee_code = f"EMP_{existing_count+1:03d}"
                        while Employee.query.filter_by(employee_code=final_employee_code).first():
                            existing_count += 1
                            final_employee_code = f"EMP_{existing_count+1:03d}"
                    
                    employee = Employee(name=employee_name, employee_code=final_employee_code)
                    db.session.add(employee)
                    db.session.flush()
                    print(f"âœ… å‰µå»ºå“¡å·¥: {employee_name} (ä»£ç¢¼: {final_employee_code})")
                elif employee_code and employee_code.strip() and employee.employee_code != employee_code:
                    # å¦‚æœæ‰¾åˆ°åŒåå“¡å·¥ä½†ä»£ç¢¼ä¸åŒï¼Œæ›´æ–°å“¡å·¥ä»£ç¢¼ï¼ˆå¦‚æœä¸è¡çªï¼‰
                    if not Employee.query.filter_by(employee_code=employee_code).first():
                        old_code = employee.employee_code
                        employee.employee_code = employee_code
                        print(f"ğŸ”„ æ›´æ–°å“¡å·¥ä»£ç¢¼: {employee_name} ({old_code} â†’ {employee_code})")
                
                # æŸ¥æ‰¾æˆ–å‰µå»ºç­åˆ¥é¡å‹ï¼ˆä½¿ç”¨åŸå§‹çš„è¤‡åˆç­åˆ¥ä»£ç¢¼ï¼‰
                final_shift_code = original_shift_code if force_import else shift_code
                shift_type = ShiftType.query.filter_by(code=final_shift_code).first()
                if not shift_type:
                    if force_import or final_shift_code == 'UNASSIGNED':
                        # å¼·åˆ¶åŒ¯å…¥æˆ–æœªæ’ç­ï¼šå‰µå»ºæ–°ç­åˆ¥é¡å‹
                        shift_name, start_time, end_time, color = get_shift_info(final_shift_code)
                        shift_type = ShiftType(
                            code=final_shift_code,
                            name=shift_name,
                            start_time=start_time,
                            end_time=end_time,
                            color=color
                        )
                        db.session.add(shift_type)
                        db.session.flush()
                        print(f"âœ… å‰µå»ºæ–°ç­åˆ¥é¡å‹: {final_shift_code}")
                    else:
                        # ä¸€èˆ¬æ¨¡å¼ï¼šä½¿ç”¨æ¨™æº–ç­åˆ¥è³‡è¨Š
                        shift_name, start_time, end_time, color = get_shift_info(shift_code)
                        shift_type = ShiftType(
                            code=shift_code,
                            name=shift_name,
                            start_time=start_time,
                            end_time=end_time,
                            color=color
                        )
                        db.session.add(shift_type)
                        db.session.flush()
                
                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨æ’ç­è¨˜éŒ„
                existing_schedule = Schedule.query.filter_by(
                    date=schedule_date,
                    employee_id=employee.id
                ).first()
                
                if existing_schedule:
                    # æ›´æ–°ç¾æœ‰è¨˜éŒ„
                    if existing_schedule.shift_type_id != shift_type.id:
                        existing_schedule.shift_type_id = shift_type.id
                        existing_schedule.updated_at = datetime.utcnow()
                        # æ›´æ–°åŒ¯å…¥é †åºå’Œæ™‚é–“æˆ³
                        existing_schedule.import_order = index + 1
                        existing_schedule.import_timestamp = datetime.utcnow()
                        updated_count += 1
                    else:
                        skipped_count += 1  # è³‡æ–™ç›¸åŒï¼Œè·³é
                else:
                    # å‰µå»ºæ–°è¨˜éŒ„
                    schedule = Schedule(
                        date=schedule_date,
                        employee_id=employee.id,
                        shift_type_id=shift_type.id,
                        import_order=index + 1,  # CSVè¡Œè™Ÿä½œç‚ºåŒ¯å…¥é †åº
                        import_timestamp=datetime.utcnow()
                    )
                    db.session.add(schedule)
                    imported_count += 1
                
            except Exception as e:
                print(f'ç¬¬{index+2}è¡ŒåŒ¯å…¥éŒ¯èª¤: {e}')
                skipped_count += 1
                continue
        
        db.session.commit()
        # æœ€çµ‚æäº¤å’Œçµ±è¨ˆ
        db.session.commit()
        print(f"âœ… æ‰¹é‡åŒ¯å…¥å®Œæˆï¼åŒ¯å…¥: {imported_count}, æ›´æ–°: {updated_count}, è·³é: {skipped_count}")
        
        return {
            'imported_count': imported_count, 
            'updated_count': updated_count,
            'skipped_count': skipped_count,
            'total_processed': imported_count + updated_count + skipped_count
        }
        
    except Exception as e:
        db.session.rollback()
        raise e

def perform_data_import(df, validation_result):
    """åŸ·è¡Œå¯¦éš›è³‡æ–™åŒ¯å…¥ï¼ˆä¿ç•™å‘å¾Œå…¼å®¹æ€§ï¼‰"""
    imported_count = 0
    skipped_count = 0
    
    try:
        name_col, date_col, shift_col = identify_columns(df)
        
        for index, row in df.iterrows():
            try:
                # æå–è³‡æ–™
                employee_name = str(row[name_col]).strip()
                date_value = row[date_col]
                shift_code = str(row[shift_col]).strip()
                
                # è·³éç©ºç™½æˆ–ç„¡æ•ˆè³‡æ–™
                if not employee_name or not shift_code or pd.isna(date_value):
                    continue
                
                # è™•ç†æ—¥æœŸ
                if isinstance(date_value, str):
                    schedule_date = datetime.strptime(date_value, '%Y-%m-%d').date()
                else:
                    schedule_date = date_value.date() if hasattr(date_value, 'date') else date_value
                
                # æŸ¥æ‰¾æˆ–å‰µå»ºå“¡å·¥
                employee = Employee.query.filter_by(name=employee_name).first()
                if not employee:
                    existing_count = Employee.query.count()
                    employee_code = f"EMP_{existing_count+1:03d}"
                    while Employee.query.filter_by(employee_code=employee_code).first():
                        existing_count += 1
                        employee_code = f"EMP_{existing_count+1:03d}"
                    
                    employee = Employee(name=employee_name, employee_code=employee_code)
                    db.session.add(employee)
                    db.session.flush()
                
                # æŸ¥æ‰¾æˆ–å‰µå»ºç­åˆ¥é¡å‹
                shift_type = ShiftType.query.filter_by(code=shift_code).first()
                if not shift_type:
                    shift_name, start_time, end_time, color = get_shift_info(shift_code)
                    shift_type = ShiftType(
                        code=shift_code,
                        name=shift_name,
                        start_time=start_time,
                        end_time=end_time,
                        color=color
                    )
                    db.session.add(shift_type)
                    db.session.flush()
                
                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨æ’ç­è¨˜éŒ„
                existing_schedule = Schedule.query.filter_by(
                    date=schedule_date,
                    employee_id=employee.id
                ).first()
                
                if existing_schedule:
                    # æ›´æ–°ç¾æœ‰è¨˜éŒ„
                    if existing_schedule.shift_type_id != shift_type.id:
                        existing_schedule.shift_type_id = shift_type.id
                        existing_schedule.updated_at = datetime.utcnow()
                        # æ›´æ–°åŒ¯å…¥é †åºå’Œæ™‚é–“æˆ³
                        existing_schedule.import_order = index + 1
                        existing_schedule.import_timestamp = datetime.utcnow()
                        updated_count += 1
                    else:
                        skipped_count += 1  # è³‡æ–™ç›¸åŒï¼Œè·³é
                else:
                    # å‰µå»ºæ–°è¨˜éŒ„
                    schedule = Schedule(
                        date=schedule_date,
                        employee_id=employee.id,
                        shift_type_id=shift_type.id,
                        import_order=index + 1,  # CSVè¡Œè™Ÿä½œç‚ºåŒ¯å…¥é †åº
                        import_timestamp=datetime.utcnow()
                    )
                    db.session.add(schedule)
                    imported_count += 1
                
            except Exception as e:
                print(f'ç¬¬{index+2}è¡ŒåŒ¯å…¥éŒ¯èª¤: {e}')
                continue
        
        db.session.commit()
        # æœ€çµ‚æäº¤å’Œçµ±è¨ˆ
        db.session.commit()
        print(f"âœ… æ‰¹é‡åŒ¯å…¥å®Œæˆï¼åŒ¯å…¥: {imported_count}, æ›´æ–°: {updated_count}, è·³é: {skipped_count}")
        
        return {
            'imported_count': imported_count, 
            'updated_count': updated_count,
            'skipped_count': skipped_count,
            'total_processed': imported_count + updated_count + skipped_count
        }
        
    except Exception as e:
        db.session.rollback()
        raise e

# æ–°è¦ç¯„çš„ä¸Šå‚³åŠŸèƒ½
@main.route("/upload_csv", methods=["POST"])
@require_auth
def upload_csv():
    """è™•ç†CSVæª”æ¡ˆä¸Šå‚³"""
    try:
        if "file" not in request.files:
            return jsonify({"error": "æ²’æœ‰é¸æ“‡æª”æ¡ˆ"}), 400
        
        file = request.files["file"]
        if file.filename == "":
            return jsonify({"error": "æ²’æœ‰é¸æ“‡æª”æ¡ˆ"}), 400
        
        if not file.filename.lower().endswith(".csv"):
            return jsonify({"error": "åªæ”¯æ´CSVæª”æ¡ˆ"}), 400
        
        # è®€å–CSVæª”æ¡ˆ
        stream = io.StringIO(file.stream.read().decode("utf-8"))
        csv_data = list(csv.DictReader(stream))
        
        # é è¦½å’Œé©—è­‰è³‡æ–™
        preview_result = preview_csv_data(csv_data)
        
        return jsonify(preview_result)
        
    except Exception as e:
        return jsonify({"error": f"æª”æ¡ˆè™•ç†éŒ¯èª¤: {str(e)}"}), 500

@main.route("/upload_pasted", methods=["POST"])
@require_auth
def upload_pasted():
    """è™•ç†è²¼ä¸Šçš„CSVè³‡æ–™"""
    try:
        data = request.json
        csv_text = data.get("csv_data", "").strip()
        
        if not csv_text:
            return jsonify({"error": "æ²’æœ‰è¼¸å…¥è³‡æ–™"}), 400
        
        # è§£æCSVæ–‡å­—
        csv_data = list(csv.DictReader(io.StringIO(csv_text)))
        
        # é è¦½å’Œé©—è­‰è³‡æ–™
        preview_result = preview_csv_data(csv_data)
        
        return jsonify(preview_result)
        
    except Exception as e:
        return jsonify({"error": f"è³‡æ–™è™•ç†éŒ¯èª¤: {str(e)}"}), 500

@main.route("/import_data", methods=["POST"])
@require_auth
def import_data():
    """åŸ·è¡Œè³‡æ–™åŒ¯å…¥"""
    try:
        data = request.json
        csv_data = data.get("data", [])
        selected_months = data.get("selected_months", [])
        
        if not csv_data:
            return jsonify({"error": "æ²’æœ‰è³‡æ–™å¯åŒ¯å…¥"}), 400
        
        # éæ¿¾é¸å®šæœˆä»½çš„è³‡æ–™
        filtered_data = []
        if selected_months:
            for row in csv_data:
                year_month = row.get("å¹´æœˆ", "")
                if year_month in selected_months:
                    filtered_data.append(row)
        else:
            filtered_data = csv_data
        
        # åŸ·è¡ŒåŒ¯å…¥
        result = import_csv_data(filtered_data)
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({"error": f"åŒ¯å…¥éŒ¯èª¤: {str(e)}"}), 500

def preview_csv_data(csv_data):
    """é è¦½CSVè³‡æ–™ä¸¦åˆ†æå¹´æœˆåˆ†ä½ˆ"""
    if not csv_data:
        return {"error": "æ²’æœ‰è³‡æ–™"}
    
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    required_fields = ["å§“å", "å“¡å·¥ä»£ç¢¼", "å¹´æœˆ", "æ—¥æœŸ", "ç­åˆ¥"]
    first_row = csv_data[0]
    missing_fields = [field for field in required_fields if field not in first_row.keys()]
    
    if missing_fields:
        missing_fields_str = ", ".join(missing_fields)
        return {"error": f"ç¼ºå°‘æ¬„ä½: {missing_fields_str}"}
    
    # åˆ†æå¹´æœˆåˆ†ä½ˆ
    month_distribution = defaultdict(int)
    preview_data = []
    errors = []
    
    for i, row in enumerate(csv_data[:50]):  # åªé è¦½å‰50ç­†
        try:
            name = row.get("å§“å", "").strip()
            employee_code = row.get("å“¡å·¥ä»£ç¢¼", "").strip()
            year_month = row.get("å¹´æœˆ", "").strip()
            day = row.get("æ—¥æœŸ", "").strip()
            shift_code = row.get("ç­åˆ¥", "").strip()
            
            if not all([name, employee_code, year_month, day, shift_code]):
                errors.append(f"ç¬¬{i+1}è¡Œè³‡æ–™ä¸å®Œæ•´")
                continue
            
            # çµ„åˆå®Œæ•´æ—¥æœŸ
            try:
                if "-" in year_month:
                    year, month = year_month.split("-")
                else:
                    year, month = year_month[:4], year_month[4:]
                
                full_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                datetime.strptime(full_date, "%Y-%m-%d")  # é©—è­‰æ—¥æœŸæ ¼å¼
                
                month_distribution[year_month] += 1
                
                preview_data.append({
                    "å§“å": name,
                    "å“¡å·¥ä»£ç¢¼": employee_code,
                    "å¹´æœˆ": year_month,
                    "æ—¥æœŸ": day,
                    "ç­åˆ¥": shift_code,
                    "å®Œæ•´æ—¥æœŸ": full_date
                })
                
            except ValueError as e:
                errors.append(f"ç¬¬{i+1}è¡Œæ—¥æœŸæ ¼å¼éŒ¯èª¤: {year_month}-{day}")
                
        except Exception as e:
            errors.append(f"ç¬¬{i+1}è¡Œè™•ç†éŒ¯èª¤: {str(e)}")
    
    return {
        "success": True,
        "preview_data": preview_data,
        "month_distribution": dict(month_distribution),
        "total_records": len(csv_data),
        "errors": errors
    }

def import_csv_data(csv_data):
    """åŒ¯å…¥CSVè³‡æ–™åˆ°è³‡æ–™åº«"""
    imported_count = 0
    updated_count = 0
    error_count = 0
    errors = []
    
    try:
        for row in csv_data:
            try:
                name = row.get("å§“å", "").strip()
                employee_code = row.get("å“¡å·¥ä»£ç¢¼", "").strip()
                year_month = row.get("å¹´æœˆ", "").strip()
                day = row.get("æ—¥æœŸ", "").strip()
                shift_code = row.get("ç­åˆ¥", "").strip()
                
                # çµ„åˆå®Œæ•´æ—¥æœŸ
                if "-" in year_month:
                    year, month = year_month.split("-")
                else:
                    year, month = year_month[:4], year_month[4:]
                
                full_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                schedule_date = datetime.strptime(full_date, "%Y-%m-%d").date()
                
                # æŸ¥æ‰¾æˆ–å‰µå»ºå“¡å·¥
                employee = Employee.query.filter_by(employee_code=employee_code).first()
                if not employee:
                    employee = Employee(name=name, employee_code=employee_code)
                    db.session.add(employee)
                    db.session.flush()
                
                # æŸ¥æ‰¾æˆ–å‰µå»ºç­åˆ¥é¡å‹
                shift_type = ShiftType.query.filter_by(code=shift_code).first()
                if not shift_type:
                    shift_type = ShiftType(
                        code=shift_code,
                        name=f"{shift_code}ç­",
                        start_time=datetime.strptime("09:00", "%H:%M").time(),
                        end_time=datetime.strptime("17:00", "%H:%M").time(),
                        color="#007bff"
                    )
                    db.session.add(shift_type)
                    db.session.flush()
                
                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨æ’ç­è¨˜éŒ„
                existing_schedule = Schedule.query.filter_by(
                    date=schedule_date,
                    employee_id=employee.id
                ).first()
                
                if existing_schedule:
                    existing_schedule.shift_type_id = shift_type.id
                    existing_schedule.updated_at = datetime.utcnow()
                    updated_count += 1
                else:
                    schedule = Schedule(
                        date=schedule_date,
                        employee_id=employee.id,
                        shift_type_id=shift_type.id
                    )
                    db.session.add(schedule)
                    imported_count += 1
                    
            except Exception as e:
                error_count += 1
                errors.append(f"è³‡æ–™è™•ç†éŒ¯èª¤: {str(e)}")
        
        db.session.commit()
        
        return {
            "success": True,
            "imported_count": imported_count,
            "updated_count": updated_count,
            "error_count": error_count,
            "errors": errors[:10]  # åªè¿”å›å‰10å€‹éŒ¯èª¤
        }
        
    except Exception as e:
        db.session.rollback()
        return {"success": False, "error": str(e)}

